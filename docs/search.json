[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Visual Statistics",
    "section": "",
    "text": "1 Introduction\nThis website is largely inspired by what our advisor, colleague, and friend Bob Pruzek called elemental graphics. The first two paragraphs from Helmreich and Pruzek (2010) succinctly introduces the core ideas of elemental graphics:\nTheir initial paper focused on a specific graphic for analysis of variance (ANOVA), which we cover with some additions. The chapters of this book attempt to take that same philosophy that there are natural graphics that exemplify important statistical concepts beyond ANOVA. This book is not intended to be a standalone introductory statistics book but instead a collection of articles we have used to support the teaching of statistics.\nThis is an online book because we have found that by having students interact with the graphics, often by controlling specific parameters and/or adding features one-by-one provides a deeper understanding of the concepts. The introduction of the Shiny (Chang et al. 2023) R package has greatly simplified the process of creating interactive visualiztions.\nAll of the Shiny applications, supporting functions, and datasets are included in the VisualStats R package. This can be downloaded using the following command:\nremotes::install_github('jbryer/VisualStats')"
  },
  {
    "objectID": "index.html#colophon",
    "href": "index.html#colophon",
    "title": "Visual Statistics",
    "section": "1.1 Colophon",
    "text": "1.1 Colophon\nThis book was creating using Quarto and hosted on Github.\n\ndevtools::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       macOS Sonoma 14.4\n system   aarch64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/New_York\n date     2024-03-26\n pandoc   3.1.1 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n cachem        1.0.8   2023-05-01 [1] CRAN (R 4.3.0)\n cli           3.6.2   2023-12-11 [1] CRAN (R 4.3.1)\n devtools      2.4.5   2022-10-11 [1] CRAN (R 4.3.0)\n digest        0.6.35  2024-03-11 [1] CRAN (R 4.3.1)\n ellipsis      0.3.2   2021-04-29 [1] CRAN (R 4.3.0)\n evaluate      0.23    2023-11-01 [1] CRAN (R 4.3.1)\n fastmap       1.1.1   2023-02-24 [1] CRAN (R 4.3.0)\n fs            1.6.3   2023-07-20 [1] CRAN (R 4.3.0)\n glue          1.7.0   2024-01-09 [1] CRAN (R 4.3.1)\n htmltools     0.5.8   2024-03-25 [1] CRAN (R 4.3.1)\n htmlwidgets   1.6.4   2023-12-06 [1] CRAN (R 4.3.1)\n httpuv        1.6.15  2024-03-26 [1] CRAN (R 4.3.1)\n jsonlite      1.8.8   2023-12-04 [1] CRAN (R 4.3.1)\n knitr         1.45    2023-10-30 [1] CRAN (R 4.3.1)\n later         1.3.2   2023-12-06 [1] CRAN (R 4.3.1)\n lifecycle     1.0.4   2023-11-07 [1] CRAN (R 4.3.1)\n magrittr      2.0.3   2022-03-30 [1] CRAN (R 4.3.0)\n memoise       2.0.1   2021-11-26 [1] CRAN (R 4.3.0)\n mime          0.12    2021-09-28 [1] CRAN (R 4.3.0)\n miniUI        0.1.1.1 2018-05-18 [1] CRAN (R 4.3.0)\n pkgbuild      1.4.4   2024-03-17 [1] CRAN (R 4.3.1)\n pkgload       1.3.4   2024-01-16 [1] CRAN (R 4.3.1)\n profvis       0.3.8   2023-05-02 [1] CRAN (R 4.3.0)\n promises      1.2.1   2023-08-10 [1] CRAN (R 4.3.0)\n purrr         1.0.2   2023-08-10 [1] CRAN (R 4.3.0)\n R6            2.5.1   2021-08-19 [1] CRAN (R 4.3.0)\n Rcpp          1.0.12  2024-01-09 [1] CRAN (R 4.3.1)\n remotes       2.4.2.1 2023-07-18 [1] CRAN (R 4.3.0)\n rlang         1.1.3   2024-01-10 [1] CRAN (R 4.3.1)\n rmarkdown     2.26    2024-03-05 [1] CRAN (R 4.3.1)\n rstudioapi    0.15.0  2023-07-07 [1] CRAN (R 4.3.0)\n sessioninfo   1.2.2   2021-12-06 [1] CRAN (R 4.3.0)\n shiny         1.8.1   2024-03-26 [1] CRAN (R 4.3.1)\n stringi       1.8.3   2023-12-11 [1] CRAN (R 4.3.1)\n stringr       1.5.1   2023-11-14 [1] CRAN (R 4.3.1)\n urlchecker    1.0.1   2021-11-30 [1] CRAN (R 4.3.0)\n usethis       2.2.2   2023-07-06 [1] CRAN (R 4.3.0)\n vctrs         0.6.5   2023-12-01 [1] CRAN (R 4.3.1)\n xfun          0.43    2024-03-25 [1] CRAN (R 4.3.1)\n xtable        1.8-4   2019-04-21 [1] CRAN (R 4.3.0)\n\n [1] /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n\nChang, Winston, Joe Cheng, JJ Allaire, Carson Sievert, Barret Schloerke, Yihui Xie, Jeff Allen, Jonathan McPherson, Alan Dipert, and Barbara Borges. 2023. Shiny: Web Application Framework for r. https://CRAN.R-project.org/package=shiny.\n\n\nHelmreich, James, and Robert Pruzek. 2010. “Elemental Graphics for Analysis of Variance Using the r Package granova.”"
  },
  {
    "objectID": "variance.html",
    "href": "variance.html",
    "title": "2  Variance",
    "section": "",
    "text": "#| label: fig-shiny-variance\n#| viewerHeight: 600\n#| standalone: true\nwebr::install(\"ggplot2\")\n\nlibrary(ggplot2)\n\nsource('https://raw.githubusercontent.com/jbryer/VisualStats/main/R/variance_vis.R')\nsource('https://raw.githubusercontent.com/jbryer/VisualStats/main/R/variance_shiny.R')\n\nshinyApp(ui = variance_shiny_ui, server = variance_shiny_server)\n\nFigure 2.1: Visualization of variance.\n\n\nThis Shiny application can be run locally using the VisualStats::variance_shiny() function."
  },
  {
    "objectID": "distributions.html#normal-distribution",
    "href": "distributions.html#normal-distribution",
    "title": "3  Distributions",
    "section": "3.1 Normal Distribution",
    "text": "3.1 Normal Distribution\n\nnormal_plot()"
  },
  {
    "objectID": "loess.html",
    "href": "loess.html",
    "title": "4  Loess Regression",
    "section": "",
    "text": "LOESS (locally estimated scatterplot smoothing) provides a method for visualizing the relationship between variables when classical linear and non-linear least squares regression may not be the most appropriate. The non-parametric technique explores the relationship between variables using local regressions using only regions of the explanatory variable (IV).\nThis shiny app permits visualization of those local regressions along the whole of the X variable space. The way that the loess curve is calculated is to use the predicted value in the response dimension (DV) for the chosen X value based on the local regression. These predicted points are visualized with the orange dot on the plot and the curve can be seen to be “drawn” when using animation of the centering slider.\nThe ‘loess’ function in R provides the capability for either first or second degree polynomial specification for the loess fit (linear or quadratic) and this shiny app provides that same choice along with the “span” specification which affects the smoothing outcome. Center and span work by locating the local regressions and determining the X axis range employed.\nThe loess algorithm uses in R and in this shiny app follow an approach developed by Cleveland (1979) and which was apparently also in use in some fields as the Savitsky-Golay filter (Savitzky and Golay 1964). It weights data points closer to the center of the localized regression more heavily than those more distanced.\nThis app does not permit choice of smoothing family. It uses the default Gaussian kernel (least squares approach) of the loess function in R.\nThe weights for each x are calculated using:\n\\[\\left( 1 - \\left(\\frac{dist}{max(dist)}\\right)^3 \\right)^3\\]\nwhen \\(\\alpha &lt; 1\\) (i.e. span).\n\ndata('faithful')\nloess_vis(formula = eruptions ~ waiting,\n          data = faithful)\n\n\n\n\n\n\n\n\n\n\n\n#| label: fig-shiny-loess\n#| viewerHeight: 600\n#| standalone: true\nwebr::install(\"ggplot2\")\nwebr::install(\"shinyWidgets\")\nwebr::install(\"shinyBS\")\nwebr::install(\"dplyr\")\nlibrary(ggplot2)\nlibrary(shinyWidgets)\nlibrary(shinyBS)\nlibrary(dplyr)\nsource('https://raw.githubusercontent.com/jbryer/VisualStats/main/R/loess_vis.R')\nsource('https://raw.githubusercontent.com/jbryer/VisualStats/main/R/loess_shiny.R')\nshinyApp(ui = loess_shiny_ui, server = loess_shiny_server)\n\nFigure 4.1: Visualization of Loess regression. This app is a visual/conceptual demonstration of the elements of a Loess Plot.The orange point plots the predicted value from an X axis value midway along a local regression shown by the local (green) regression fit. The center of this locally weighted regression and its X axis span can be controlled by the CENTER slider and SPAN numeric entry box. Best use of the app would use the animation control for the CENTER slider and the checkbox for actively drawing the Loess fit. The full loess fit can also be displayed with the second checkbox. The local regression can be specifed as a linear or quadratic fit with the DEGREE dropdown.\n\n\n\n\n\n\nCleveland, W. S. 1979. “Robust Locally Weighted Regression and Smoothing Scatterplots.” Journal of the American Statistical Association 74.\n\n\nSavitzky, A., and M. J. E. Golay. 1964. “Smoothing and Differentiation of Data by Simplified Least Squares Procedures.” Analytical Chemistry 36."
  },
  {
    "objectID": "mle.html#ordinary-least-squares",
    "href": "mle.html#ordinary-least-squares",
    "title": "5  Maximum Likelihood Estimation",
    "section": "5.1 Ordinary Least Squares",
    "text": "5.1 Ordinary Least Squares\nWith ordinary least squares (OLS) regression our goal is to minimize the residual sum of squares (RSS):\n\\[RSS=\\sum^{n}_{i=1} \\left( y_{i}-f(x_{i})\\right)^{2}\\]\nwhere \\(y_i\\) is the variable to be predicted, \\(f(x_i)\\) is the predicted value of \\(y_i\\), and \\(n\\) is the sample size. Figure 2 superimposes the residuals on the scatter plot. By squaring the length of each of those lines we accomplish two things: 1) we make all the values to be summed positive (i.e. a line that fits all the data perfectly will have a RSS = 0) and 2) points that fall further from the regression line contribute more (geometrically more) to the RSS then points close to the regression line.\n\n\n\n\n\nFigure 2 Scatter plot with residuals.\n\n\n\n\nThe basic properties we know about regression are:\n\nThe correlation measures the strength of the relationship between x and y (see this shiny app for an excellent visual overview of correlations).\nThe correlation ranges between -1 and 1.\nThe mean of x and y must fall on the line.\nThe slope of a line is defined as the change in y over the change in x (\\(\\frac{\\Delta y}{\\Delta x}\\)). For regression use the ration of the standard deviations such that the correlation is defined as \\(m = r \\frac{s_y}{s_x}\\) where \\(m\\) is the slope, \\(r\\) is the correlation, and \\(s\\) is the sample standard deviation.\n\nWe can easily calculate the RSS for various correlations (\\(r\\)) ranging between -1 and 1. Figure 3 visualizes the RSS.\n\ny &lt;- mtcars$mpg\nx &lt;- mtcars$wt\nmean.y &lt;- mean(y)\nmean.x &lt;- mean(x)\nsd.y &lt;- sd(y)\nsd.x &lt;- sd(x)\nols &lt;- tibble(\n    r = seq(-1, 1, by = 0.025),            # Correlation\n    m = r * (sd.y / sd.x),                 # Slope\n    b = mean.y - m * mean.x                # Intercept\n) %&gt;% rowwise() %&gt;%\n    mutate(ss = sum((y - (m * x + b))^2)) %&gt;% # Sum of squares residuals\n    as.data.frame()\ndatatable(ols) %&gt;% formatRound(columns = names(ols), digits=3)\n\n\n\n\n\n\n\n\n\n\n\nFigure 3. Residual sum of squares.\n\n\n\n\nThe correlation with the correlation the resulted in the smallest RSS is -0.875.\n\nols %&gt;% dplyr::filter(ss == min(ss)) # Select the row with the smallest sum of squares residuals\n\n       r         m       b       ss\n1 -0.875 -5.389687 37.4306 278.3826\n\n\nCalculating the correlation in R gives us -0.8676594 and the slope is -5.3444716 which is close to our estimate here. We could get a more accurate result if we tried smaller steps in the correlation (see the by parameter in the seq function above)."
  },
  {
    "objectID": "mle.html#minimizing-rss-algorithmically",
    "href": "mle.html#minimizing-rss-algorithmically",
    "title": "5  Maximum Likelihood Estimation",
    "section": "5.2 Minimizing RSS Algorithmically",
    "text": "5.2 Minimizing RSS Algorithmically\nThis approach works well here because the correlation is bounded between -1 and 1 and we can easily calculate the RSS for a bunch of possible correlations. However, there are more efficient ways of finding the correlation that minimizes the RSS than trying correlations equally distributed across the possible range. For example, consider the following simple algorithm:\n\nCalculate the RSS for \\(r = 0\\).\nCalculate the RSS for \\(r = 0.5\\) If \\(RSS_{0.5} &lt; RSS_{0}\\) then calculate the RSS with \\(r = 0.75\\), else calculate the RSS with \\(r = -0.5%\\)\n\nWe can repeat this procedure, essentially halving the distance in each iteration until we find a sufficiently small RSS. This process is, in essence, the idea of numerical optimization procedures. In R, the optim function implements the Nedler-Mead (Nedler & Mead, 1965) and Limited Memory BFGS (Byrd et al, 1995) methods for optimizing a set of parameters. The former is the default but we will use the latter throughout this document since it allows for specifying bounds for certain parameters (e.g. only consider positive values). The details of how the algorithm works is beyond the scope of this article (see this interactive tutoral by Ben Frederickson for a good introduction), instead we will focus on what the algorithm does. To begin, we must define a function that calculates a metric for which the optimizer is going to minimize (or maximize). Let’s start with RSS:\n\nresidual_sum_squares &lt;- function(parameters, predictor, outcome) {\n    a &lt;- parameters[1] # Intercept\n    b &lt;- parameters[2] # beta coefficient\n    predicted &lt;- a + b * predictor\n    residuals &lt;- outcome - predicted\n    ss &lt;- sum(residuals^2)\n    return(ss)\n}\n\nThe parameters is a vector of the parameters the algorithm is going to minimize (or maximize). Here, these will be the slope and intercept. The predictor and outcome are parameters passed through from the ... parameter on the optim function and are necessary for us to calculate the RSS. We can now get the RSS for any set of parameters.\n\nresidual_sum_squares(c(37, -5), mtcars$wt, mtcars$mpg)\n\n[1] 303.5247\n\n\nSmall Digression In order to explore each step of the algorithm, we need to wrap the optim function to capture the parameters and output of the function. The optim_save1 function will add two elements to the returned list: iterations is the raw list of the parameters and output saved and iterations_df is a data.frame containing the same data.\n\noptim_save &lt;- function(par, fn, ...) {\n    iterations &lt;- list()\n    wrap_fun &lt;- function(parameters, ...) {\n        n &lt;- length(iterations)\n        result &lt;- fn(parameters, ...)\n        iterations[[n + 1]] &lt;&lt;- c(parameters, result)\n        return(result)\n    }\n    optim_out &lt;- stats::optim(par, wrap_fun, ...)\n    optim_out$iterations &lt;- iterations\n    optim_out$iterations_df &lt;- as.data.frame(do.call(rbind, iterations))\n    names(optim_out$iterations_df) &lt;- c(paste0('Param', 1:length(par)), 'Result')\n    optim_out$iterations_df$Iteration &lt;- 1:nrow(optim_out$iterations_df)\n    return(optim_out)\n}\n\nWe can now call the optim_save function with our residual_sum_squares function. We initialize the algorithm with two random values for the intercept and slope, respectively. Note that we are using Broyden, Fletcher, Goldfarb, and Shanno optimization method which allows for the specification of bounds on the parameter estimates which we will use later.\n\noptim.rss &lt;- optim_save(\n    par = runif(2),\n    fn = residual_sum_squares, \n    method = \"L-BFGS-B\",\n    predictor = mtcars$wt,\n    outcome = mtcars$mpg\n)\n\nThe par parameter provides the final parameter estimates.\n\noptim.rss$par\n\n[1] 37.285116 -5.344469\n\n\nWe can see that the parameters are accurate to at least four decimal places to the OLS method used by the lm function.\n\nlm.out &lt;- lm(mpg ~ wt, data = mtcars)\nlm.out$coefficients\n\n(Intercept)          wt \n  37.285126   -5.344472 \n\n\nIt took the optim function 65 iterations to find the optimal set of parameters that minimized the RSS. Figure 4 shows the value of the parameters (i.e. intercept and slope) and the RSS for each iteration.\n\ndf &lt;- optim.rss$iterations_df\nnames(df) &lt;- c('Intercept', 'Slope', 'ResidualSumSquares', 'Iteration')\ndf %&gt;% melt(id.var = 'Iteration') %&gt;%\n    ggplot(aes(x = Iteration, y = value, color = variable)) +\n    geom_point(size = 1) + geom_path() +\n    facet_wrap(~ variable, scales = \"free_y\", ncol = 1) +\n    xlab('Iteration') + ylab('') + theme(legend.position = 'none')\n\n\n\n\nFigure 4. Output of the optimizaiton procedure at each iteration."
  },
  {
    "objectID": "mle.html#likelihood",
    "href": "mle.html#likelihood",
    "title": "5  Maximum Likelihood Estimation",
    "section": "5.3 Likelihood",
    "text": "5.3 Likelihood\nNow that we have laid the groundwork for finding parameters algorithmically, we need to introduce another way of evaluating how well parameters fit the data, namely the likelihood. First, let’s revisit what we are doing in OLS. Figure 5 is a scatter plot of our observations, the OLS regression line in blue, and one observation highlighted in red with the residual as a red line. With OLS, we square the residual for every observation, thereby making all values positive, and summing them. There is, however, another way of estimating fit that doesn’t rely on the residuals.\n\n\n\n\n\nFigure 5. Scatter plot with residuals for one observation.\n\n\n\n\nWe often think of probabilities as the areas under a fixed distribution. For example, the first car in mtcars is Mazda RX4 with an average miles per gallon of 21 and weighs 2620lbs. The probability of a car with a miles per gallon less than Mazda RX4 given the data we have in mtcars is 0.5599667 and is depicted in Figure 6.\n\n\n\n\n\nFigure 6. Probability distribution of miles per gallan.\n\n\n\n\nFor probabilities, we are working with a fixed distribution, that is:\n\\[pr(data\\ |\\ distribution)\\] The likelihood are the y-axis values (i.e. density) for fixed data points with distributions that can move, that is:\n\\[L(distribution\\ |\\ data)\\] The likelihood is the height of the density function. Figure 7 depicts two likelihood for two observations. The mean of each distribution is equal to \\(\\beta_{wt} X + e\\) and the intercept (also known as the error term) defines the standard deviation of the distribution.\n\n\n\n\n\nFigure 7. Likelihood of a car having the observed mpg given the model parameters for two observations.\n\n\n\n\nWe can then calculate the likelihood for each observation in our data. Unlike OLS, we now want to maximize the sum of these values. Also, we are going to use the log of the likelihood so we can add them instead of multiplying. We can now define our log likelihood function:\n\nloglikelihood &lt;- function(parameters, predictor, outcome) {\n    a &lt;- parameters[1]     # intercept\n    b &lt;- parameters[2]     # slope / beta coefficient\n    sigma &lt;- parameters[3] # error\n    ll.vec &lt;- dnorm(outcome, a + b * predictor, sigma, log = TRUE)\n    return(sum(ll.vec))\n}\n\nNote that we have to estimate a third parameter, sigma, which is the error term and defines the standard deviation for the normal distribution for estimating the likelihood. This is connected to the distribution of the residuals as we will see later. We can now calculate the log-likelihood for any combination of parameters.\n\nloglikelihood(c(37, -5, sd(mtcars$mpg)),\n              predictor = mtcars$wt,\n              outcome = mtcars$mpg)\n\n[1] -91.06374"
  },
  {
    "objectID": "mle.html#maximum-likelihood-estimation",
    "href": "mle.html#maximum-likelihood-estimation",
    "title": "5  Maximum Likelihood Estimation",
    "section": "5.4 Maximum Likelihood Estimation",
    "text": "5.4 Maximum Likelihood Estimation\nWe can now use the optim_save function to find the parameters that maximize the log-likelihood. Note two important parameter changes:\n\nWe are specifying the lower parameter so that the algorithm will not try negative values for sigma since the variance cannot be negative.\nThe value for the control parameter indicates that we wish to maximize the values instead of minimizing (which is the default).\n\n\noptim.ll &lt;- optim_save(\n    runif(3),                     # Random initial values\n    loglikelihood,                # Log-likelihood function\n    lower = c(-Inf, -Inf, 1.e-5), # The lower bounds for the values, note sigma (error), cannot be negative\n    method = \"L-BFGS-B\",\n    control = list(fnscale = -1), # Indicates that the maximum is desired rather than the minimum\n    predictor = mtcars$wt,\n    outcome = mtcars$mpg\n)\n\nWe can get our results and compare them to the results of the lm function and find that they match to at least four decimal places.\n\noptim.ll$par[1:2]\n\n[1] 37.285114 -5.344468\n\nlm.out$coefficients\n\n(Intercept)          wt \n  37.285126   -5.344472 \n\n\nFigure 8 shows the estimated regression line for each iteration of the optimization procedure (on the left; OLS regression line in blue; MLE regression line in black) with the estimated parameters and log-likelihood for all iterations on the left.\nFigure 9 superimposes the normal distribution from which the log-likelihood is determined. The distribution is centered on \\(\\hat{y}\\). The height of the distribution (i.e. density) at \\(y\\) is the likelihood. We take the log of this value to get the log-likelihood. These log-likelihoods are calculated for each observation and summed. Maximum likelihood estimation is attempting to find the parameters (i.e. slope and intercept) that maximizes the log-likelihood.\n\nVisualStats::plot_likelihood(x = mtcars$wt, \n                           y = mtcars$mpg,\n                           pt = 2,\n                           intercept = optim.ll$par[1],\n                           slope = optim.ll$par[2],\n                           sigma = optim.ll$par[3])\n\n\n\n\nFigure 9. Likelihood for one observeration superimposed on scatter plot.\n\n\n\n\nFigure 10 depicts the likelihoods for the first 16 observations.\n\ndf &lt;- optim.ll$iterations_df\nnames(df) &lt;- c('Intercept', 'Slope', 'Sigma', 'LogLikelihood', 'Iteration')\ntmp &lt;- df %&gt;% dplyr::filter(Iteration == nrow(df))\nplots &lt;- list()\nnplots &lt;- 16 #nrow(mtcars)\nfor(i in 1:min(nplots, nrow(mtcars))) {\n    a &lt;- tmp[1,]$Intercept\n    b &lt;- tmp[1,]$Slope\n    sigma &lt;- tmp[1,]$Sigma\n    predictor &lt;- mtcars$wt[i]\n    predicted.out &lt;- a + b * predictor\n    outcome &lt;- mtcars$mpg[i]\n    d &lt;- dnorm(outcome, predicted.out, sigma)\n    plots[[i]] &lt;- ggplot() +\n        stat_function(fun = dnorm,\n                      n = 101,\n                      args = list(mean = predicted.out, sd = sigma)) +\n        annotate(geom = 'segment', x = outcome, y = 0, xend = outcome, yend = d, color = 'red') +\n        annotate(geom = 'point', x = outcome, y = d, color = 'red', size = 2) +\n        xlim(c(min(mtcars$mpg, predicted.out - 3 * sigma),\n               max(mtcars$mpg, predicted.out + 3 * sigma))) +\n        ylim(c(0, .2)) +\n        ylab('') + xlab(row.names(mtcars)[i])\n}\nplot_grid(plotlist = plots)\n\n\n\n\nFigure 10. Likelihoods of the first 16 observations for the final parameter estimates.\n\n\n\n\nWith MLE we need to estimate what is often referred to as the error term, or as we saw above is the standard deviation of the normal distribution from which we are estimating the likelihood from. In Figure 9 notice that the normal distribution id drawn vertically. This is because the likelihood is estimated from the error, or the residuals. In OLS we often report the root-mean-square deviation (RMSD, or root-mean-square error, RMSE). The RMSD is the standard deviation of the residuals:\n\\[RMSD\\  =\\  \\sqrt{\\frac{\\sum^{N}_{i=1} (x_{i}-\\hat{x_{i}} )^{2}}{N} }\\] Where \\(i\\) is the observation, \\(x_i\\) is the observed value, \\(\\hat{x_i}\\) is the estimated (predicted) value, and \\(N\\) is the sample size. Below, we see that the numerical optimizer matches the RMSD within a rounding error.\n\noptim.ll$par[3]\n\n[1] 2.949164\n\nsqrt(sum(resid(lm.out)^2) / nrow(mtcars))\n\n[1] 2.949163"
  },
  {
    "objectID": "mle.html#generalized-linear-models",
    "href": "mle.html#generalized-linear-models",
    "title": "5  Maximum Likelihood Estimation",
    "section": "5.5 Generalized Linear Models",
    "text": "5.5 Generalized Linear Models\nGeneralized linear models (GLM) are a generalization of OLS that allows for the response variables (i.e. dependent variables) to have an error distribution that is not distributed normally. All generalized linear models have the following three characteristics:\n\nA probability distribution describing the outcome variable .\nA linear model\n\\(\\eta = \\beta_0+\\beta_1 X_1 + \\cdots + \\beta_n X_n\\).\nA link function that relates the linear model to the parameter of the outcome distribution\n\\(g(p) = \\eta\\) or \\(p = g^{-1}(\\eta)\\).\n\nWe can estimate GLMs using MLE as described above. What will change is the log-likelihood function."
  },
  {
    "objectID": "mle.html#logistic-regression",
    "href": "mle.html#logistic-regression",
    "title": "5  Maximum Likelihood Estimation",
    "section": "5.6 Logistic Regression",
    "text": "5.6 Logistic Regression\nLogistic regression is a GLM used to model a binary categorical variable using numerical and categorical predictors. We assume a binomial distribution produced the outcome variable and we therefore want to model p the probability of success for a given set of predictors. Instead of fitting a line (or a plane for two predictors, etc. for higher dimensions) we wish to fit the data to the logistic function which is defined as:\n\\[ \\sigma \\left( t \\right) =\\frac { { e }^{ t } }{ { e }^{ t }+1 } =\\frac { 1 }{ 1+{ e }^{ -t } }  \\]\n\nlogistic &lt;- function(t) { \n    return(1 / (1 + exp(-t))) \n}\nggplot() +\n    stat_function(fun = logistic, n = 101) +\n    xlim(-4, 4) + xlab('x')\n\n\n\n\nFigure 11. Logistic curve\n\n\n\n\nTo finish specifying the Logistic model we just need to establish a reasonable link function that connects \\(\\eta\\) to \\(p\\). There are a variety of options but the most commonly used is the logit function which is specified as:\n\\[logit(p) = \\log\\left(\\frac{p}{1-p}\\right),\\text{ for $0\\le p \\le 1$}\\]\nWe can specify t as a linear combination of our predictors (independent variables).\n\\[ t = \\beta_0 + \\beta_1 x \\]\nThe logistic function can now be rewritten as:\n\\[ F\\left( x \\right) =\\frac { 1 }{ 1+{ e }^{ -\\left( { \\beta  }_{ 0 }+\\beta _{ 1 }x \\right)  } } \\]\nConsider the following data set where we wish to predict whether a student will pass an exam based upon the number of hours they studied.2.\n\nstudy &lt;- data.frame(\n    Hours=c(0.50,0.75,1.00,1.25,1.50,1.75,1.75,2.00,2.25,2.50,2.75,3.00,\n            3.25,3.50,4.00,4.25,4.50,4.75,5.00,5.50),\n    Pass=c(0,0,0,0,0,0,1,0,1,0,1,0,1,0,1,1,1,1,1,1)\n)\n\n\n\n\n\n\nFigure 12. Boxplot of hours studied by passing.\n\n\n\n\nFirst, we need to define logit function and the log-likelihood function that will be used by the optim function. Instead of using the normal distribution as above (using the dnorm function), we are using a binomial distribution and the logit to link the linear combination of predictors.\n\nlogit &lt;- function(x, beta0, beta1) {\n    return( 1 / (1 + exp(-beta0 - beta1 * x)) )\n}\nloglikelihood.binomial &lt;- function(parameters, predictor, outcome) {\n    a &lt;- parameters[1] # Intercept\n    b &lt;- parameters[2] # beta coefficient\n    p &lt;- logit(predictor, a, b)\n    ll &lt;- sum( outcome * log(p) + (1 - outcome) * log(1 - p))\n    return(ll)\n}\n\nNow we can call the optim function and get the final parameter estimates.\n\noptim.binomial &lt;- optim_save(\n    c(0, 1), # Initial values\n    loglikelihood.binomial,\n    method = \"L-BFGS-B\",\n    control = list(fnscale = -1),\n    predictor = study$Hours,\n    outcome = study$Pass\n)\n\nIn R, the glm (short for generalized linear models) function implements logistic regression when the family = binomial(link = 'logit') parameter is set. See ?glm for other families of models to estimate models with other underlying distributions. We can see that our estimate matches the results of glm to a rounding error.\n\noptim.binomial$par\n\n[1] -4.077575  1.504624\n\nlr.out &lt;- glm(Pass ~ Hours, data = study, family = binomial(link = 'logit'))\nlr.out$coefficients\n\n(Intercept)       Hours \n  -4.077713    1.504645 \n\n\n\n# Redefine the logistic function to include parameter estimates\nlogistic &lt;- function(x, beta0, beta1) {\n    return(1 / (1 + exp(-1 * (beta0 + beta1 * x)) ))\n}\n\nbeta0 &lt;- optim.binomial$par[1]\nbeta1 &lt;- optim.binomial$par[2]\n\nggplot(study, aes(x = Hours, y = Pass)) +\n    geom_point(aes(color = logistic(Hours, beta0, beta1) &gt; 0.5)) +\n    stat_function(fun = logistic, n = 101, \n                  args = list(beta0 = beta0, beta1 = beta1) ) +\n    scale_color_hue('Predicted Pass &gt; 0.5') +\n    theme(legend.position = 'inside', legend.position.inside = c(0.85, 0.15))\n\n\n\n\n\n\n\n\nLet’s explore the process of the numeric optimizer. For this model, it took 70 iterations to converge to resulting parameters.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#| label: fig-shiny-variance\n#| viewerHeight: 600\n#| standalone: true\n\nwebr::install(\"mnormt\")\nwebr::install(\"ggplot2\")\nwebr::install(\"dplyr\")\nwebr::install(\"psych\")\nwebr::install(\"tidyr\")\nwebr::install(\"cowplot\")\nwebr::install(\"reshape2\")\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(psych)\nlibrary(tidyr)\nlibrary(cowplot)\nlibrary(reshape2)\n\nsource('https://raw.githubusercontent.com/jbryer/VisualStats/main/R/isBinary.R')\nsource('https://raw.githubusercontent.com/jbryer/VisualStats/main/R/optim_save.R')\nsource('https://raw.githubusercontent.com/jbryer/VisualStats/main/R/likelihood_normal.R')\nsource('https://raw.githubusercontent.com/jbryer/VisualStats/main/R/likelihood_binomial.R')\n\n\nsource('https://raw.githubusercontent.com/jbryer/VisualStats/main/R/variance_vis.R')\nsource('https://raw.githubusercontent.com/jbryer/VisualStats/main/R/mle_shiny.R')\n\nshinyApp(ui = variance_shiny_ui, server = variance_shiny_server)\n\nFigure 5.1: Visualization of maximum likelihood estimation.\n\n\nThis Shiny application can be run locally using the VisualStats::mle_shiny() function."
  },
  {
    "objectID": "mle.html#footnotes",
    "href": "mle.html#footnotes",
    "title": "5  Maximum Likelihood Estimation",
    "section": "",
    "text": "This function is available in the VisualStats package.↩︎\nThese data were retrived from the Wikipedia article on logistic regresion↩︎"
  },
  {
    "objectID": "anova.html#footnotes",
    "href": "anova.html#footnotes",
    "title": "6  Analysis of Variance (ANOVA)",
    "section": "",
    "text": "The anova_vis function specifies ggplot2::coord_equal on all plots.↩︎\nSee https://web.ma.utexas.edu/users/mks/M358KInstr/SampleSDPf.pdf for a detailed explanation.↩︎"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Chang, Winston, Joe Cheng, JJ Allaire, Carson Sievert, Barret Schloerke,\nYihui Xie, Jeff Allen, Jonathan McPherson, Alan Dipert, and Barbara\nBorges. 2023. Shiny: Web Application Framework for r. https://CRAN.R-project.org/package=shiny.\n\n\nCleveland, W. S. 1979. “Robust Locally Weighted Regression and\nSmoothing Scatterplots.” Journal of the American Statistical\nAssociation 74.\n\n\nHelmreich, James, and Robert Pruzek. 2010. “Elemental Graphics for\nAnalysis of Variance Using the r Package granova.”\n\n\nSavitzky, A., and M. J. E. Golay. 1964. “Smoothing and\nDifferentiation of Data by Simplified Least Squares Procedures.”\nAnalytical Chemistry 36."
  }
]