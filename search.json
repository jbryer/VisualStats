[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Visual Statistics",
    "section": "",
    "text": "1 Introduction\nLast updated on September 30, 2025\nThis website is largely inspired by what our advisor, colleague, and friend Bob Pruzek called elemental graphics. The first two paragraphs from Pruzek and Helmreich (2010) succinctly introduces the core ideas of elemental graphics:\nTheir initial paper focused on a specific graphic for analysis of variance (ANOVA), which we cover with some additions. The chapters of this book attempt to take that same philosophy that there are natural graphics that exemplify important statistical concepts beyond ANOVA. This book is not intended to be a standalone introductory statistics book but instead a collection of articles we have used to support the teaching of statistics.\nThis is an online book because we have found that by having students interact with the graphics, often by controlling specific parameters and/or adding features one-by-one provides a deeper understanding of the concepts. The introduction of the Shiny (Chang et al. 2023) R package has greatly simplified the process of creating interactive visualiztions.\nAll of the Shiny applications, supporting functions, and data sets are included in the VisualStats R package. This can be downloaded using the following command:\nremotes::install_github('jbryer/VisualStats')\nThis website/book does not intend to be another introductory statistics book, there are plenty of fantastic options available, but instead a collection of Shiny applications and chapters to support those visualizations. Our philosophy is that students can better understand the statistical formulas instrumental to the field itself through geometric representations. Take for example the fact that many statistical formulas square terms. Drawing a square for these formulas becomeas a natural way of exploring the features of what that statistical formula is attempting to solve.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#how-to-use-this-book",
    "href": "index.html#how-to-use-this-book",
    "title": "Visual Statistics",
    "section": "1.1 How to use this book",
    "text": "1.1 How to use this book\nThis book is not intended to be a standalone introduction to statistics. There are a lot of high quality, open access, resources and textbooks for learning statistics. Many of these visualizations were created to supplement the OpenIntro textbook. Instead, this book and accompanying R package are designed to provide a more visual exploration of important statistical concepts.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#contributions",
    "href": "index.html#contributions",
    "title": "Visual Statistics",
    "section": "1.2 Contributions",
    "text": "1.2 Contributions\nThis website is a work in progress. If you have suggestions and/or edits you can either open an issue on Github or click “edit this page” at the bottom of each page.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#colophon",
    "href": "index.html#colophon",
    "title": "Visual Statistics",
    "section": "1.3 Colophon",
    "text": "1.3 Colophon\nThis book was creating using Quarto (Allaire and Dervieux 2024) and hosted on Github. The interactive components were created using Shiny (Chang et al. 2023). All of the figures were created using the grammar of graphics (Wilkinson 2005) framework as implemented by the ggplot2 package (Wickham 2016).\n\ndevtools::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.3 (2024-02-29)\n os       Ubuntu 24.04.3 LTS\n system   x86_64, linux-gnu\n ui       X11\n language (EN)\n collate  C.UTF-8\n ctype    C.UTF-8\n tz       UTC\n date     2025-09-30\n pandoc   3.1.3 @ /usr/bin/ (via rmarkdown)\n quarto   1.8.25 @ /usr/local/bin/quarto\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n cachem        1.1.0   2024-05-16 [1] RSPM\n cli           3.6.5   2025-04-23 [1] RSPM\n devtools      2.4.5   2022-10-11 [1] any (@2.4.5)\n digest        0.6.37  2024-08-19 [1] RSPM\n ellipsis      0.3.2   2021-04-29 [1] RSPM\n evaluate      1.0.5   2025-08-27 [1] RSPM\n fastmap       1.2.0   2024-05-15 [1] RSPM\n fs            1.6.6   2025-04-12 [1] RSPM\n glue          1.8.0   2024-09-30 [1] RSPM\n htmltools     0.5.8.1 2024-04-04 [1] RSPM\n htmlwidgets   1.6.4   2023-12-06 [1] RSPM\n httpuv        1.6.16  2025-04-16 [1] RSPM\n jsonlite      2.0.0   2025-03-27 [1] RSPM\n knitr         1.50    2025-03-16 [1] RSPM\n later         1.4.4   2025-08-27 [1] RSPM\n lifecycle     1.0.4   2023-11-07 [1] RSPM\n magrittr      2.0.4   2025-09-12 [1] RSPM\n memoise       2.0.1   2021-11-26 [1] RSPM\n mime          0.13    2025-03-17 [1] RSPM\n miniUI        0.1.2   2025-04-17 [1] RSPM\n pkgbuild      1.4.8   2025-05-26 [1] RSPM\n pkgload       1.4.1   2025-09-23 [1] RSPM\n profvis       0.4.0   2024-09-20 [1] RSPM\n promises      1.3.3   2025-05-29 [1] RSPM\n purrr         1.1.0   2025-07-10 [1] RSPM\n R6            2.6.1   2025-02-15 [1] RSPM\n Rcpp          1.1.0   2025-07-02 [1] RSPM\n remotes       2.5.0   2024-03-17 [1] RSPM\n rlang         1.1.6   2025-04-11 [1] RSPM\n rmarkdown     2.30    2025-09-28 [1] RSPM\n sessioninfo   1.2.3   2025-02-05 [1] any (@1.2.3)\n shiny         1.11.1  2025-07-03 [1] RSPM\n urlchecker    1.0.1   2021-11-30 [1] RSPM\n usethis       3.2.1   2025-09-06 [1] RSPM\n vctrs         0.6.5   2023-12-01 [1] RSPM\n xfun          0.53    2025-08-19 [1] RSPM\n xtable        1.8-4   2019-04-21 [1] RSPM\n yaml          2.3.10  2024-07-26 [1] RSPM\n\n [1] /home/runner/work/_temp/Library\n [2] /opt/R/4.3.3/lib/R/site-library\n [3] /opt/R/4.3.3/lib/R/library\n\n──────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n\nAllaire, JJ, and Christophe Dervieux. 2024. Quarto: R Interface to ’Quarto’ Markdown Publishing System. https://CRAN.R-project.org/package=quarto.\n\n\nChang, Winston, Joe Cheng, JJ Allaire, Carson Sievert, Barret Schloerke, Yihui Xie, Jeff Allen, Jonathan McPherson, Alan Dipert, and Barbara Borges. 2023. Shiny: Web Application Framework for r. https://CRAN.R-project.org/package=shiny.\n\n\nPruzek, Robert, and James Helmreich. 2010. “Elemental Graphics for Analysis of Variance Using the r Package granova.”\n\n\nWickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org.\n\n\nWilkinson, Leland. 2005. The Grammar of Graphics (Statistics and Computing). Berlin, Heidelberg: Springer-Verlag.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "descriptives.html",
    "href": "descriptives.html",
    "title": "2  Descriptive Statistics",
    "section": "",
    "text": "3 Mean versus median",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "descriptives.html#sec-variance",
    "href": "descriptives.html#sec-variance",
    "title": "2  Descriptive Statistics",
    "section": "3.1 Variance",
    "text": "3.1 Variance\n\n\nShow the code\nx &lt;- c(97.88, 107.91, 88.26, 115.21, 87.38)\n\n\n\\[ s^2 = \\frac{\\Sigma(x_i - \\bar{x})^2}{n - 1}  \\tag{3.1}\\]\n\n\nShow the code\nvariance_vis(x, plot_deviances_x = 4, plot_deviances = FALSE, plot_population_variance = FALSE) +\n    ylim(c(-1, 5)) + theme(axis.text.y = element_blank()) +\n    annotate('text', x = mean(x) + (max(x) - mean(x)) / 2, y = 4,\n             label = bquote(\"x - bar(x)\"), parse = TRUE, vjust = 1.5, size = 8)\n\n\n\n\n\n\n\n\nFigure 3.1: Deviation for the largest value.\n\n\n\n\n\n\n\nShow the code\nvariance_vis(x, plot_deviances_x = 4, \n             plot_deviances = 4, \n             plot_population_variance = FALSE) +\n    ylim(c(0, 20))\n\n\n\n\n\n\n\n\nFigure 3.2: Squared deviation for the largest value.\n\n\n\n\n\n\n\nShow the code\nvariance_vis(x, \n             plot_deviances = TRUE, \n             plot_population_variance = FALSE) +\n    ylim(c(0, 20))\n\n\n\n\n\n\n\n\nFigure 3.3: Squared deviation for all observations.\n\n\n\n\n\n\n\nShow the code\nvariance_vis(x, \n             plot_deviances = TRUE, \n             plot_population_variance = TRUE,\n             plot_sample_variance = TRUE) + \n    ylim(c(0,35))\n\n\n\n\n\n\n\n\nFigure 3.4: Squared deviation for all observations along with population and sample variances.\n\n\n\n\n\n\\[ s = \\sqrt{s^2} = \\sqrt{\\frac{\\sigma(x_i - \\bar{x})^2}{n - 1}}  \\tag{3.2}\\]\n\n3.1.1 Sample versus population variance\nWe used the population variance above because the denominator, N, is easier to demonstrate since the variance is simply the average of the area of all the squares. However, we will almost always want to estimate the sample variance which changes the denominator to n - 1 (technically called the degrees of freedom which will be described in more detail in later chapters). In fact, the var function in R to calculate variance will always return the sample variance (and sd function for standard deviation by extension). In practice reporting the sample variance when working with a population can be considered a slightly conservative estimate. However, as n increases, the difference between the sample and population variances are going to converge (see Appendix A for more details about convergence, limits, and core calculus concepts helpful for learning statistics).\nLet’s first define two function to caclulate the sample and population variances.\n\n\nShow the code\nsample_var &lt;- function(x) {\n    mean_x &lt;- mean(x)\n    n &lt;- length(x)\n    sum( (x - mean_x)^2) / (n - 1)\n}\n\npopulation_var &lt;- function(x) {\n    mean_x &lt;- mean(x)\n    N &lt;- length(x)\n    sum( (x - mean_x)^2) / N\n}\n\n\nWe will now calculate the sample and population variance from a vector randomly generated from the normal distribution with mean of 0 and standard deviation of 1.\n\n\nShow the code\nmax_n &lt;- 500\nx &lt;- rnorm(max_n)\nvariances &lt;- data.frame(n = seq(2, max_n, 1),\n                        sample_var = numeric(max_n - 1),\n                        population_var = numeric(max_n - 1))\nfor(i in seq_len(nrow(variances))) {\n    n &lt;- variances[i,]$n\n    variances[i,]$sample_var &lt;- sample_var(x[1:n])\n    variances[i,]$population_var &lt;- population_var(x[1:n])\n}\nvariances &lt;- variances |&gt;\n    dplyr::mutate(Difference = abs(population_var - sample_var))\n\n\nFigure @ref(fig:fig-variance-convergence) depicts the two variances along with the difference for sample sizes rangine from 2 to 500. The blue line representing the difference is very close to zero when n = 200.\n\n\nShow the code\nvariances |&gt; \n    reshape2::melt(id.vars = 'n', variable.name = 'Estimate', value.name = 'Variance') |&gt;\n    ggplot(aes(x = n, y = Variance, color = Estimate)) +\n        geom_path()\n\n\n\n\n\n\n\n\nFigure 3.5: Sample versus population variances as n increases.\n\n\n\n\n\nExamining the last five rows (i.e. n = 496 to 500) show that the difference in the two variances is less than 0.01.\n\n\nShow the code\ntail(variances, 5)\n\n\n      n sample_var population_var  Difference\n495 496  0.9842646      0.9822802 0.001984404\n496 497  0.9836034      0.9816243 0.001979081\n497 498  0.9827055      0.9807322 0.001973304\n498 499  0.9811142      0.9791481 0.001966161\n499 500  0.9792802      0.9773217 0.001958560\n\n\n \nThis Shiny application can be run locally using the VisualStats::variance_shiny() function.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "distributions.html",
    "href": "distributions.html",
    "title": "3  Distributions",
    "section": "",
    "text": "3.1 Normal Distribution\nWhen working with distributions in R, each distribution has four functions, namely:\nWhere XXX is the distribution name (e.g. norm, binom, t, etc.).\nThe VisualStats::plot_distributions() function will generate four plots representing the four R distribution functions. For each subplot points correspond to the first parameter of the corresponding function (note the subplot for the random rXXX function does not have points since this simply returns random values from that distribution). The arrows correspond to what that function will return.\nThe top two plots (dXXX and rXXX) plot the distribution. The bottom two plots are the cumulative density function for the given distribution. The CDF describes the probability that a random variable (X) will be less than or equal to a specific value (x), written as F(x) = P(X ≤ x). The CDF provides a complete view of a random variable’s distribution by accumulating probabilities up to that point.\nHere is a full list of available distributions (note that the links go to the corresponding Wikipedia article).\nThe distributions_shiny() function will launch a Shiny application that provides an interface to use the plot_distributions() function interactively.\nShow the code\nnormal_plot(mean = 0, sd = 1, cv = c(-1.96, 1.96))\n\n\n\n\n\n\n\n\nFigure 3.1: Normal distribution with 95% of the area.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Distributions</span>"
    ]
  },
  {
    "objectID": "distributions.html#sec-qq-plot",
    "href": "distributions.html#sec-qq-plot",
    "title": "3  Distributions",
    "section": "3.2 Comparing Distributions: Quantile-Quantile Plots",
    "text": "3.2 Comparing Distributions: Quantile-Quantile Plots\nThe quantile-quantile plot (Q-Q plot) is a graphical method for comparing two distributions. This is done by comparing the quantiles1 between the two distributions. Q-Q plots are often used to determine whether the distribution of a sample approximates the normal distribution, though it should be noted that it can be used for any type of distribution. To illustrate how the Q-Q plot is constructed, let’s begin with three populations with a uniform, skewed, and normal distribution.\n\n\nShow the code\npop_size &lt;- 100000\nsamp_size &lt;- 50\ndistributions &lt;- data.frame(\n    unif_pop = runif(pop_size),\n    skew_pop = rchisq(pop_size, df = 5),\n    norm_pop = rnorm(pop_size, mean = 2, sd = 1)\n)\n\n\n\n\nShow the code\ndistributions |&gt;\n    reshape2::melt(variable.name = 'distribution') |&gt; \n    ggplot(aes(x = value, color = distribution)) + \n    geom_density() + \n    facet_wrap(~ distribution, scales = 'free', ncol = 1) +\n    theme_vs()\n\n\n\n\n\nPopulation Distributions\n\n\n\n\nLet’s start with a small sample with n = 6.\n\n\nShow the code\nsamp1 &lt;- sample(distributions$skew_pop, size = 6)\nsamp1\n\n\n[1] 6.605251 2.147276 6.244270 6.036563 6.184984 4.255935\n\n\nIf we wish to determine if our sample approximates the normal distribution, we randomly select values (using n = 6) from the normal distribution using the rnorm function.\n\n\nShow the code\nsamp_norm &lt;- rnorm(length(samp1))\nsamp_norm\n\n\n[1] -0.1160746 -0.8244852 -0.4644825  0.2672684  0.6679687 -0.6274922\n\n\nThe Q-Q plot plots the theoretical quantiles (using the rnorm function in this example) against the sample quantiles. We take the two vectors, sort them, then combine them such that the smallest value from the sample is paired with the smallest value from the theoretical distribution (i.e. rnorm here), all the way to the largest values.\n\n\nShow the code\nsamp1 &lt;- samp1 |&gt; sort()\nsamp_norm &lt;- samp_norm |&gt; sort()\ncbind(samp1, samp_norm)\n\n\n        samp1  samp_norm\n[1,] 2.147276 -0.8244852\n[2,] 4.255935 -0.6274922\n[3,] 6.036563 -0.4644825\n[4,] 6.184984 -0.1160746\n[5,] 6.244270  0.2672684\n[6,] 6.605251  0.6679687\n\n\nThe following scatter plot depicts the foundation of the Q-Q plot. If the two distributions are the same then we would expect all the points to fall on a straight line.\n\n\nShow the code\nggplot(cbind(samp1, samp_norm), aes(x = samp1, y = samp_norm)) + \n    geom_point() + \n    theme_vs()\n\n\n\n\n\n\n\n\n\nIt is desirable to draw a straight line for reference, however determining the slope and intercept requires some work. If both the sample and comparison distributions are expressed as standard scores (i.e. mean = 0, standard deviation = 1), then we can draw the unit line (i.e. \\(y = x\\)).\n\n\nShow the code\nggplot(cbind(samp1, samp_norm), aes(x = scale(samp1), y = scale(samp_norm))) + \n    geom_abline(slope = 1, intercept = 0) +\n    geom_point() + \n    theme_vs()\n\n\n\n\n\n\n\n\n\nHowever, if we wish to retain the scaling of the original sample, we need an alternative strategy to determine the slope and intercept of the line. One common approach is to take the paired quantiles at the 25th and 75th percentiles and then calculate the equation of the line that intercepts those two points.\n\n\nShow the code\nx_points &lt;- quantile(samp1, probs = c(0.25, 0.75))\nx_points\n\n\n     25%      75% \n4.701092 6.229449 \n\n\nShow the code\ny_points &lt;- quantile(samp_norm, probs = c(0.25, 0.75))\ny_points\n\n\n       25%        75% \n-0.5867398  0.1714326 \n\n\nShow the code\nslope &lt;- diff(y_points) / diff(x_points)\nintercept &lt;- y_points[1] - slope * x_points[1]\n\nggplot(cbind(samp1, samp_norm), aes(x = samp1, y = samp_norm)) + \n    geom_abline(slope = slope, intercept = intercept) +\n    geom_point() + \n    theme_vs()\n\n\n\n\n\n\n\n\n\nAn alternative variation to the Q-Q plot line would be to plot a Loess regression line. If the two distributions are the same then the Loess regression line should be a straight line. With very small samples as demonstrated below, however, the Loess estimation is not very good.\n\n\nShow the code\nggplot(cbind(samp1, samp_norm), aes(x = samp1, y = samp_norm)) + \n    geom_abline(slope = slope, intercept = intercept) +\n    geom_smooth(formula = y ~ x, method = 'loess', se = FALSE, span = 1) +\n    geom_point() + \n    theme_vs()\n\n\n\n\n\n\n\n\n\nLet’s draw random samples of n = 50 from the three populations defined above.\n\n\nShow the code\nunif_samp &lt;- sample(distributions$unif_pop, size = samp_size)\nskew_pop &lt;- sample(distributions$skew_pop, size = samp_size)\nnorm_pop &lt;- sample(distributions$norm_pop, size = samp_size)\n\n\nThe following three figures are Q-Q plots comparing the sample distributions to the normal distribution using the gg_qq_plot function in the VisualStats package. This is slight variation on the traditional Q-Q plot by also plotting the marginal distributions.\n\n\nShow the code\ngg_qq_plot(unif_samp, loess = TRUE)\n\n\n\n\n\nNormal Q-Q Plot of Sample from a Uniform Population Distribution\n\n\n\n\n\n\nShow the code\ngg_qq_plot(skew_pop, loess = TRUE)\n\n\n\n\n\nNormal Q-Q Plot of Sample from a Skewed Population Distribution\n\n\n\n\n\n\nShow the code\ngg_qq_plot(norm_pop, loess = TRUE)\n\n\n\n\n\nNormal Q-Q Plot of Sample from Normal Population Distributed\n\n\n\n\nAlthough the most common use of Q-Q plots is to determine whether a sample distribution approximates the normal distribution, technically the Q-Q plot allows for the comparison between any two distributions. For example, the skewed distribution example created above was randomly selected from a chi-squared distribution. The theoretical_dist parameter of the gg_qq_plot function allows you to change the comparison distribution.\n\n\nShow the code\ngg_qq_plot(skew_pop, theoretical_dist = 'chisq', df = 5, \n           loess = TRUE)\n\n\n\n\n\nChi-Squared Q-Q Plot",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Distributions</span>"
    ]
  },
  {
    "objectID": "distributions.html#footnotes",
    "href": "distributions.html#footnotes",
    "title": "3  Distributions",
    "section": "",
    "text": "Quantiles are simply cut points from a distribution such that the intervals have equal probabilities.↩︎",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Distributions</span>"
    ]
  },
  {
    "objectID": "correlation.html",
    "href": "correlation.html",
    "title": "4  Correlation",
    "section": "",
    "text": "Correlation is a measure of the relationship between two variables. The correlation can range from -1 indicating a “perfect” negative relationship to 1 indicating a “perfect” positive relationship. A correlation of 0 indicates no relationship. Figure 4.1 depicts scatter plots with various correlations.\n\n\nShow the code\nplots &lt;- list()\nfor(rho in c(-1, -0.8, -0.4, 0, 0.4, 0.8, 1)) {\n    df &lt;- mvtnorm::rmvnorm(n = 30,\n                           mean = c(0, 0),\n                           sigma = matrix(c(1^2, rho * (1 * 1),\n                                         rho * (1 * 1), 1^2), 2, 2)) |&gt;\n        as.data.frame()\n    \n    plots[[length(plots) + 1]] &lt;- ggplot(df, aes(x = V1, y = V2)) + \n        geom_point(size = 0.5) +\n        theme_minimal() +\n        theme(axis.text = element_blank(), panel.grid = element_blank()) +\n        xlab(rho) + ylab('') +\n        coord_equal() +\n        xlim(c(-3, 3)) + ylim(c(-3, 3))\n}\nplots$nrow &lt;- 1\ndo.call(plot_grid, plots)\n\n\n\n\n\n\n\n\nFigure 4.1: Scatterplots representing correlations from -1 to 1.\n\n\n\n\n\nFor a population, the correlation is defined as the ratio of the covariance to the product of the standard deviations, and is typically denoted using the Greek letter rho (\\(\\rho\\)), is defined as:\n\\[ \\rho = \\frac{cov(X,Y)}{\\sigma_{X} \\sigma_{Y}}  \\tag{4.1}\\]\nThe standard deviation (\\(\\sigma\\)) is equal to the square root of the variance (\\(\\sigma = \\sqrt{\\frac{\\Sigma(x_i - \\bar{x})^2}{n - 1}}\\)) which is covered in detail in Section 3.1. What is new here is the covariance. Like variance, we are interested in deviations from the mean except now in two dimensions. The formula for the covariance is:\n\\[ cov_{xy} = \\frac{\\Sigma(x_i - \\bar{x})(y_i - \\bar{y})}{n - 1}  \\tag{4.2}\\]\nLet’s break down Equation 4.2 visually. The following R code generates a data frame with two variables, x and y, with means of 20 and 40 and standard deviatiosn of 2 and 3, respectively. The population correlation is 0.8 (note that the sample will likely have a slightly different correlation).\n\nmean_x &lt;- 20\nmean_y &lt;- 40\nsd_x &lt;- 2\nsd_y &lt;- 3\nn &lt;- 30\nrho &lt;- 0.8\nset.seed(2112)\ndf &lt;- mvtnorm::rmvnorm(\n    n = n,\n    mean = c(mean_x, mean_y),\n    sigma = matrix(c(sd_x^2, rho * (sd_x * sd_y),\n                     rho * (sd_x * sd_y), sd_y^2), 2, 2)) |&gt;\n    as.data.frame() |&gt;\n    dplyr::rename(x = V1, y = V2) |&gt;\n    dplyr::mutate(x_deviation = x - mean(x),\n                  y_deviation = y - mean(y),\n                  cross_product = x_deviation * y_deviation)\n\nThe numerator of the covariance equation included the product of the deviation from the mean in the x direction (i.e. \\(x_i - \\bar{x}\\)) and the y direction (i.e. \\(y_i - \\bar{y}\\)). Figure 4.2 depicts the scatter plot with all 30 observations, the mean of x and y as dashed lines, and the deviations for one observation represented by blue arrows.\n\n\nShow the code\ndeviation_to_plot &lt;- 23\nregression_vis(df,\n                plot_x_mean = TRUE,\n                plot_y_mean = TRUE,\n                plot_positive_cross_products = FALSE,\n                plot_negative_cross_products = FALSE,\n                plot_x_deviations = deviation_to_plot,\n                plot_y_deviations = deviation_to_plot) +\n    annotate('text', x = 22.5, y = 46.2,\n             label = bquote(\"x - bar(x)\"), parse = TRUE, vjust = 1.5, size = 8) +\n    annotate('text', x = 25.0, y = 44,\n             label = bquote(\"y - bar(y)\"), parse = TRUE, vjust = 2, size = 8, angle = -90)\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\nℹ The deprecated feature was likely used in the VisualStats package.\n  Please report the issue at &lt;https://github.com/jbryer/VisualStats/issues&gt;.\n\n\nWarning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\nℹ The deprecated feature was likely used in the VisualStats package.\n  Please report the issue at &lt;https://github.com/jbryer/VisualStats/issues&gt;.\n\n\n\n\n\n\n\n\nFigure 4.2: Scatter plot showing the deviations for x and y.\n\n\n\n\n\nHence, for each observation (i) in our data frame the numerator is summing the area of a rectangle (see Figure 4.3 for the one observation in yellow). When we calculate variances we square the deviation which as a result ensures that all the values being summed are positive. However, for covariance this is not the case. The numbers in Figure 4.3 correspond to the four quadrants of the plot. For points that fall in quadrants 1 and 3, the cross products (i.e. \\((x_i - \\bar{x})(y_i - \\bar{y}\\)) will be positive. For points that fall in quadrants 2 and 4 however, the cross products will be negative.\n\n\nShow the code\nregression_vis(df,\n                plot_x_mean = TRUE,\n                plot_y_mean = TRUE,\n                plot_positive_cross_products = FALSE,\n                plot_negative_cross_products = FALSE,\n                plot_cross_product = deviation_to_plot,\n                plot_x_deviations = deviation_to_plot,\n                plot_y_deviations = deviation_to_plot,\n                cross_product_alpha = 0.5) +\n    geom_text(label = '1', x = 22.5, y = 44, color = 'blue', size = 16) +\n    geom_text(label = '2', x = 17.5, y = 44, color = 'blue', size = 16) +\n    geom_text(label = '3', x = 17.5, y = 36, color = 'blue', size = 16) +\n    geom_text(label = '4', x = 22.5, y = 36, color = 'blue', size = 16)\n\n\n\n\n\n\n\n\nFigure 4.3: Scatter plot showing the deviations for x and y with the cross product represented as the area of the yellow rectangle.\n\n\n\n\n\nFigure 4.4 depicts all the cross products but shades the cross products that are positive in blue and cross products that are negative in red. As a result we can interpret the covariance as the ratio of the area of cross products for points in quadrants 1 and 3 to the area of the cross products for points in quadrants 2 and 4.\n\n\nShow the code\nregression_vis(df,\n                plot_x_mean = TRUE,\n                plot_y_mean = TRUE,\n                plot_positive_cross_products = TRUE,\n                plot_negative_cross_products = TRUE)\n\n\n\n\n\n\n\n\nFigure 4.4: Scatter plot with all cross products. Positive and negative cross products are represented by blue and red rectangles, respectively.\n\n\n\n\n\nWe can see this more clearly if we plot a histogram of cross products (see Figure 4.5).\n\n\nShow the code\nggplot(df, aes(x = cross_product, fill = cross_product &gt; 0)) +\n    geom_histogram(bins = 15, alpha = 0.75) +\n    scale_fill_manual('Cross product &gt; 0', values = c('lightblue', 'darkred')) +\n    xlab('Cross Product') + \n    theme_vs()\n\n\n\n\n\n\n\n\nFigure 4.5: Histogram of cross products.\n\n\n\n\n\nThe last part of Equation 4.2 is the denominator where we average across all of the observations. If we wish to calculate the covariance for a population we divide my n, but for sample covariance we divide by n - 1. Substuting Equation 4.2 into Equation 4.1 we get the following:\n\\[{ r }_{ xy }=\\frac { \\frac { \\sum _{ i=1 }^{ n }{ \\left( { X }_{ i }-\\overline { X }  \\right) \\left( { Y }_{ i }-\\overline { Y }  \\right)  }  }{ n-1 }  }{ { s }_{ x }{ s }_{ y } } \\tag{4.3}\\]\nThe Shiny application allows you to play with all of the features that go into calculating correlation. Clicking on individual points will display the cross product.\n \nThis Shiny application can be run locally using the VisualStats::correlation_shiny() function.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "dependent_sample.html",
    "href": "dependent_sample.html",
    "title": "5  Dependent Samples",
    "section": "",
    "text": "Show the code\ndata(anorexia.sub, package = 'granova')\ndependent_sample_vis(\n    df = anorexia.sub,\n    test = 'norm', # or 't'\n    conf_level = 0.95,\n    plot_mean = FALSE,\n    plot_unit_line = TRUE,\n    plot_projections = FALSE,\n    plot_differences = FALSE,\n    plot_ci = FALSE,\n    plot_ci_lines = FALSE,\n    plot_samp_dist = FALSE\n)\n\n\nWarning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\nℹ The deprecated feature was likely used in the VisualStats package.\n  Please report the issue at &lt;https://github.com/jbryer/VisualStats/issues&gt;.\n\n\n\n\n\n\n\n\n\n\n\nShow the code\ndependent_sample_vis(\n    df = anorexia.sub,\n    test = 'norm', # or 't'\n    conf_level = 0.95,\n    plot_mean = FALSE,\n    plot_unit_line = TRUE,\n    plot_projections = TRUE,\n    plot_differences = TRUE,\n    plot_ci = FALSE,\n    plot_ci_lines = FALSE,\n    plot_samp_dist = FALSE\n)\n\n\n\n\n\n\n\n\n\n\n\nShow the code\ndependent_sample_vis(\n    df = anorexia.sub,\n    test = 'norm', # or 't'\n    conf_level = 0.95,\n    plot_mean = FALSE,\n    plot_unit_line = TRUE,\n    plot_projections = TRUE,\n    plot_differences = TRUE,\n    plot_ci = FALSE,\n    plot_ci_lines = FALSE,\n    plot_samp_dist = TRUE\n)\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\nℹ The deprecated feature was likely used in the VisualStats package.\n  Please report the issue at &lt;https://github.com/jbryer/VisualStats/issues&gt;.\n\n\n\n\n\n\n\n\n\n\n\nShow the code\ndependent_sample_vis(\n    df = anorexia.sub,\n    test = 'norm', # or 't'\n    conf_level = 0.95,\n    plot_mean = TRUE,\n    plot_unit_line = TRUE,\n    plot_projections = TRUE,\n    plot_differences = TRUE,\n    plot_ci = TRUE,\n    plot_ci_lines = FALSE,\n    plot_samp_dist = TRUE\n)\n\n\n\n\n\n\n\n\n\n \nThis Shiny application can be run locally using the VisualStats::dependent_sample_shiny() function.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Dependent Samples</span>"
    ]
  },
  {
    "objectID": "anova.html",
    "href": "anova.html",
    "title": "6  Analysis of Variance (ANOVA)",
    "section": "",
    "text": "Analysis of variance (ANOVA) remains an important statistical method for a variety of situations. ANOVA is typically taught along with other null hypothesis tests (e.g. t-test, chi-squared), however, there are many nuances differences in how the test statistic is calculated that many students do not fully understand. Specifically, why use the ratio of mean square between (or treatment) and mean square within (or error) as the test statistic? Pruzek and Helmreich (2010) introduced a visualization that attempts visualize all the components of ANOVA. This visualization was implemented in the granova (2014) R package utilizing the base graphics system. This document introduces a version of the granova visualization usinge ggplot2 Wickham (2016) that provides a number parameters to isoloate certain features of their visualization. We use this approach in teaching ANOVA, mostly through the use of Shiny (Chang et al, 2021) that allows the user/students to interactively add and remove plot features.\nTo exemplify this appraoch we will explore the effects of different approaches of hand-washing has on bacteria counts (De Veaux et al, 2009). Specifically, each of four types of hand-washing (alcohol spray, antibacterial soap, regular soap, and water only) were replicated eight times resulting in 32 observations. The boxplot below provides a summary of the data.\nWhen conceptualizing variances graphically, we wish to draw squares. Consider the formula for the sample variance:\n\\[s^2 = \\frac{\\sum(x_i - \\bar{x})^2}{n - 1}\\]\nIf we focus on the numerator for a singe observation, say \\(i = 1\\), \\((x_1 - \\bar{x})^2\\), the contribution to the variance calculation for the first observation can be conceptualized as a square with edges of length \\(x_i - \\bar{x}\\). Therefore, the sample variance (which we can also call mean square total - which ignores group membership) is approximately the average of the area of all those squares (see the figure below). NOTE: It is approximate because the sample variance has \\(n-1\\) in the denominator. The true average, using the population variances, would have \\(n\\) in the denominator. This visualization of squares sets up delineation of the visualization of components of ANOVA.\nIn order to use squares to represent the partitioned variance components in ANOVA, it is convenient to derive the deviation of each group mean from the grand mean. This aligns with using “effect coding” to code for group membership in linear modeling. Some times this kind of coding is labeled “deviation coding”. These deviations form a type of contrast and we can visualize one for each group. Consider the summary statistics for each group.\nThe mean of the dependent variable is 88.25. If we subtract this value from each group mean, then the deviance in the x-axis will be proportional to each groups deviance from the grand mean.\nThe following figure plots the dependent variable (bacterial counts) in the y-axis and each group is located at a position on the x-axis defined by the deviation for that group mean (the deviations must sum to zero).\nAn important advantage of using deviation contrasts is that a one unit change in the x-axis is the same as a one unit change in the y-axis. We can verify this by adding the unit line (i.e. \\(y = x\\)) to the plot and see that the line intersects mean of each group. We also added the grand, overall, mean in both the x and y-axes. The dashed lines represent the grand mean in two ways: 1. raw scale grand mean in the y-axis and 2. the deviation scale in the x-axis.1\nThe following table has the formulas for each component of an ANOVA summary table.\nWith k being each group and i the observation within group k, the calculation for the within group component involves calculating the deviance from the group mean. In the figure below, the vertical line for each group represents one standard deviation and the area of the box is the variance (\\(s^2\\); recall that the standard deviation is equal the square root of the variance).\nThe mean square within is approximately the average of each groups variances and is represented below as the orange square in the center. Like the variance estimate above, becasue the denominator is \\(k - 1\\) the mean square within will be slightly larger than if we used \\(k\\) in the denominator. By using \\(k - 1\\) we get an unbiased estimate.2\nSince we have equal group sizes, the pooled standard deviation (here represented by the horizontal dashed blue lines) is approximately to the square root of the mean square between (the difference in these is due to the using the degrees of freedom in the denominator instead of n). This will not necessarily be the case when the group sizes are not equal since we are taking a weighted average (note the *\\(n_k\\) in the formula for the sum of squares between).\nThere are two ways to approach the between group (treatment): 1. Since we know the total sum of squares and the within group sum of squares, we can subtract the within group sum of squares from the total to get the between group sum of squares, or 2. Perform the calculation as presented in the table above. Looking at the fomrula, we see that the between group sum of squares is centered on each group mean’s deviance from the grand mean: \\((\\bar{x}_k - \\bar{x})^2\\) (note that \\(\\bar{x}_k\\) is the group mean and \\(\\bar{x}\\) is the grand mean). The figure below depicts these deviances.\nUnlike the within group sum of squares, each deviance is multiplied by the group size, \\(n_k\\). Since we have equal group sizes in this example, they will each contribute equally to the total between group sum of squares, though it should be noted that this is not case with unequal group sizes. As a result, the mean square between is not an average of the deviances, but rather the sum of the deviances multiplied by \\(\\frac{n_k}{k - 1}\\) (in this example it is \\(\\frac{8}{3}\\)). The resulting mean square between, depicted below as the green square, is going to be larger than all the individual components of the sum of squares.\nNow that we have a square representing both the mean square between (in green) and the mean square within (in yellow), the F-statistic is equal to the ratio of the area of these two squares.\nThe p-value is the area under the F-distribution to the right of the F-statistic (repsrented as a vertical dashed line below).\nThis Shiny application can be run locally using the VisualStats::anova_shiny() function.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "anova.html#footnotes",
    "href": "anova.html#footnotes",
    "title": "6  Analysis of Variance (ANOVA)",
    "section": "",
    "text": "The anova_vis function specifies ggplot2::coord_equal on all plots.↩︎\nSee https://web.ma.utexas.edu/users/mks/M358KInstr/SampleSDPf.pdf for a detailed explanation.↩︎",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "ols_regression.html",
    "href": "ols_regression.html",
    "title": "7  OLS Regression",
    "section": "",
    "text": "mean_x &lt;- 20\nmean_y &lt;- 40\nsd_x &lt;- 2\nsd_y &lt;- 3\nn &lt;- 30\nrho &lt;- 0.8\nset.seed(2112)\ndf &lt;- mvtnorm::rmvnorm(n = n,\n                       mean = c(mean_x, mean_y),\n                       sigma = matrix(c(sd_x^2, rho * (sd_x * sd_y),\n                                     rho * (sd_x * sd_y), sd_y^2), 2, 2)) |&gt;\n    as.data.frame() |&gt;\n    dplyr::mutate(cross_products = abs(V1 * V2))\nlm_out &lt;- lm(V1 ~ V2, data = df)\ndf$prediction &lt;- predict(lm_out)\ndf$residual &lt;- resid(lm_out)\n\n\n\nShow the code\nregression_vis(df, plot_regression = TRUE, plot_residuals = df$residual == max(df$residual))\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\nℹ The deprecated feature was likely used in the VisualStats package.\n  Please report the issue at &lt;https://github.com/jbryer/VisualStats/issues&gt;.\n\n\nWarning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\nℹ The deprecated feature was likely used in the VisualStats package.\n  Please report the issue at &lt;https://github.com/jbryer/VisualStats/issues&gt;.\n\n\n\n\n\nScatter plot with residual.\n\n\n\n\n\n\nShow the code\nregression_vis(df, plot_regression = TRUE,\n               plot_residuals = df$residual == max(df$residual),\n               plot_residuals_squared = df$residual == max(df$residual))\n\n\n\n\n\nScatter plot with residual (red line) and squared residual (green squares).\n\n\n\n\n\n\nShow the code\nregression_vis(df, plot_regression = TRUE,\n               plot_residuals = TRUE,\n               plot_residuals_squared = TRUE)\n\n\n\n\n\nScatter plot with all residuals (red line) and squared residuals (green squares).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>OLS Regression</span>"
    ]
  },
  {
    "objectID": "mle.html",
    "href": "mle.html",
    "title": "8  Maximum Likelihood Estimation",
    "section": "",
    "text": "8.1 Ordinary Least Squares\nMaximum Likelihood Estimation (MLE) is an important procedure for estimating parameters in statistical models. It is often first encountered when modeling a dichotomous outcome variable vis-à-vis logistic regression. However, it is the backbone of generalized linear models (GLM) which allow for error distribution models other than the normal distribution. Most introductions to MLE rely on mathematical notation that for many students is opaque and hinders learning how this method works. The document outlines an approach to understanding MLE that relies on visualizations and mathematical notation is only used when necessary.\nWe will begin with a typical bivariate regression using the mtcars data set where we wish to predict mpg (miles per gallon) from wt (weight in 1,000 lbs). Figure 1 is a scatter plot showing the relationship between these two variables.\nOur goal is to estimate\n\\[Y_{mpg} = \\beta_{wt} X + e\\] where \\(\\beta_{wt}\\) is the slope and \\(e\\) is the intercept.\nWith ordinary least squares (OLS) regression our goal is to minimize the residual sum of squares (RSS):\n\\[RSS=\\sum^{n}_{i=1} \\left( y_{i}-f(x_{i})\\right)^{2}\\]\nwhere \\(y_i\\) is the variable to be predicted, \\(f(x_i)\\) is the predicted value of \\(y_i\\), and \\(n\\) is the sample size. Figure 2 superimposes the residuals on the scatter plot. By squaring the length of each of those lines we accomplish two things: 1) we make all the values to be summed positive (i.e. a line that fits all the data perfectly will have a RSS = 0) and 2) points that fall further from the regression line contribute more (geometrically more) to the RSS then points close to the regression line.\nFigure 2 Scatter plot with residuals.\nThe basic properties we know about regression are:\nWe can easily calculate the RSS for various correlations (\\(r\\)) ranging between -1 and 1. Figure 3 visualizes the RSS.\nShow the code\ny &lt;- mtcars$mpg\nx &lt;- mtcars$wt\nmean.y &lt;- mean(y)\nmean.x &lt;- mean(x)\nsd.y &lt;- sd(y)\nsd.x &lt;- sd(x)\nols &lt;- tibble(\n    r = seq(-1, 1, by = 0.025),            # Correlation\n    m = r * (sd.y / sd.x),                 # Slope\n    b = mean.y - m * mean.x                # Intercept\n) |&gt; rowwise() |&gt;\n    mutate(ss = sum((y - (m * x + b))^2)) |&gt; # Sum of squares residuals\n    as.data.frame()\ndatatable(ols) |&gt; formatRound(columns = names(ols), digits=3)\nFigure 3. Residual sum of squares.\nThe correlation with the correlation the resulted in the smallest RSS is -0.875.\nShow the code\nols |&gt; dplyr::filter(ss == min(ss)) # Select the row with the smallest sum of squares residuals\n\n\n       r         m       b       ss\n1 -0.875 -5.389687 37.4306 278.3826\nCalculating the correlation in R gives us -0.8676594 and the slope is -5.3444716 which is close to our estimate here. We could get a more accurate result if we tried smaller steps in the correlation (see the by parameter in the seq function above).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "mle.html#ordinary-least-squares",
    "href": "mle.html#ordinary-least-squares",
    "title": "8  Maximum Likelihood Estimation",
    "section": "",
    "text": "The correlation measures the strength of the relationship between x and y (see this shiny app for an excellent visual overview of correlations).\nThe correlation ranges between -1 and 1.\nThe mean of x and y must fall on the line.\nThe slope of a line is defined as the change in y over the change in x (\\(\\frac{\\Delta y}{\\Delta x}\\)). For regression use the ration of the standard deviations such that the correlation is defined as \\(m = r \\frac{s_y}{s_x}\\) where \\(m\\) is the slope, \\(r\\) is the correlation, and \\(s\\) is the sample standard deviation.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "mle.html#minimizing-rss-algorithmically",
    "href": "mle.html#minimizing-rss-algorithmically",
    "title": "8  Maximum Likelihood Estimation",
    "section": "8.2 Minimizing RSS Algorithmically",
    "text": "8.2 Minimizing RSS Algorithmically\nThis approach works well here because the correlation is bounded between -1 and 1 and we can easily calculate the RSS for a bunch of possible correlations. However, there are more efficient ways of finding the correlation that minimizes the RSS than trying correlations equally distributed across the possible range. For example, consider the following simple algorithm:\n\nCalculate the RSS for \\(r = 0\\).\nCalculate the RSS for \\(r = 0.5\\) If \\(RSS_{0.5} &lt; RSS_{0}\\) then calculate the RSS with \\(r = 0.75\\), else calculate the RSS with \\(r = -0.5%\\)\n\nWe can repeat this procedure, essentially halving the distance in each iteration until we find a sufficiently small RSS. This process is, in essence, the idea of numerical optimization procedures. In R, the optim function implements the Nedler-Mead (Nedler & Mead, 1965) and Limited Memory BFGS (Byrd et al, 1995) methods for optimizing a set of parameters. The former is the default but we will use the latter throughout this document since it allows for specifying bounds for certain parameters (e.g. only consider positive values). The details of how the algorithm works is beyond the scope of this article (see this interactive tutoral by Ben Frederickson for a good introduction), instead we will focus on what the algorithm does. To begin, we must define a function that calculates a metric for which the optimizer is going to minimize (or maximize). Let’s start with RSS:\n\n\nShow the code\nresidual_sum_squares &lt;- function(parameters, predictor, outcome) {\n    a &lt;- parameters[1] # Intercept\n    b &lt;- parameters[2] # beta coefficient\n    predicted &lt;- a + b * predictor\n    residuals &lt;- outcome - predicted\n    ss &lt;- sum(residuals^2)\n    return(ss)\n}\n\n\nThe parameters is a vector of the parameters the algorithm is going to minimize (or maximize). Here, these will be the slope and intercept. The predictor and outcome are parameters passed through from the ... parameter on the optim function and are necessary for us to calculate the RSS. We can now get the RSS for any set of parameters.\n\n\nShow the code\nresidual_sum_squares(c(37, -5), mtcars$wt, mtcars$mpg)\n\n\n[1] 303.5247\n\n\nSmall Digression In order to explore each step of the algorithm, we need to wrap the optim function to capture the parameters and output of the function. The optim_save1 function will add two elements to the returned list: iterations is the raw list of the parameters and output saved and iterations_df is a data.frame containing the same data.\n\n\nShow the code\noptim_save &lt;- function(par, fn, ...) {\n    iterations &lt;- list()\n    wrap_fun &lt;- function(parameters, ...) {\n        n &lt;- length(iterations)\n        result &lt;- fn(parameters, ...)\n        iterations[[n + 1]] &lt;&lt;- c(parameters, result)\n        return(result)\n    }\n    optim_out &lt;- stats::optim(par, wrap_fun, ...)\n    optim_out$iterations &lt;- iterations\n    optim_out$iterations_df &lt;- as.data.frame(do.call(rbind, iterations))\n    names(optim_out$iterations_df) &lt;- c(paste0('Param', 1:length(par)), 'Result')\n    optim_out$iterations_df$Iteration &lt;- 1:nrow(optim_out$iterations_df)\n    return(optim_out)\n}\n\n\nWe can now call the optim_save function with our residual_sum_squares function. We initialize the algorithm with two random values for the intercept and slope, respectively. Note that we are using Broyden, Fletcher, Goldfarb, and Shanno optimization method which allows for the specification of bounds on the parameter estimates which we will use later.\n\n\nShow the code\noptim.rss &lt;- optim_save(\n    par = runif(2),\n    fn = residual_sum_squares, \n    method = \"L-BFGS-B\",\n    predictor = mtcars$wt,\n    outcome = mtcars$mpg\n)\n\n\nThe par parameter provides the final parameter estimates.\n\n\nShow the code\noptim.rss$par\n\n\n[1] 37.285116 -5.344469\n\n\nWe can see that the parameters are accurate to at least four decimal places to the OLS method used by the lm function.\n\n\nShow the code\nlm.out &lt;- lm(mpg ~ wt, data = mtcars)\nlm.out$coefficients\n\n\n(Intercept)          wt \n  37.285126   -5.344472 \n\n\nIt took the optim function 65 iterations to find the optimal set of parameters that minimized the RSS. Figure 4 shows the value of the parameters (i.e. intercept and slope) and the RSS for each iteration.\n\n\nShow the code\ndf &lt;- optim.rss$iterations_df\nnames(df) &lt;- c('Intercept', 'Slope', 'ResidualSumSquares', 'Iteration')\ndf |&gt; melt(id.var = 'Iteration') |&gt;\n    ggplot(aes(x = Iteration, y = value, color = variable)) +\n    geom_point(size = 1) + geom_path() +\n    facet_wrap(~ variable, scales = \"free_y\", ncol = 1) +\n    xlab('Iteration') + ylab('') + \n    theme_vs() + theme(legend.position = 'none')\n\n\n\n\n\nFigure 4. Output of the optimizaiton procedure at each iteration.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "mle.html#likelihood",
    "href": "mle.html#likelihood",
    "title": "8  Maximum Likelihood Estimation",
    "section": "8.3 Likelihood",
    "text": "8.3 Likelihood\nNow that we have laid the groundwork for finding parameters algorithmically, we need to introduce another way of evaluating how well parameters fit the data, namely the likelihood. First, let’s revisit what we are doing in OLS. Figure 5 is a scatter plot of our observations, the OLS regression line in blue, and one observation highlighted in red with the residual as a red line. With OLS, we square the residual for every observation, thereby making all values positive, and summing them. There is, however, another way of estimating fit that doesn’t rely on the residuals.\n\n\n\n\n\nFigure 5. Scatter plot with residuals for one observation.\n\n\n\n\nWe often think of probabilities as the areas under a fixed distribution. For example, the first car in mtcars is Mazda RX4 with an average miles per gallon of 21 and weighs 2620lbs. The probability of a car with a miles per gallon less than Mazda RX4 given the data we have in mtcars is 0.5599667 and is depicted in Figure 6.\n\n\n\n\n\nFigure 6. Probability distribution of miles per gallan.\n\n\n\n\nFor probabilities, we are working with a fixed distribution, that is:\n\\[pr(data\\ |\\ distribution)\\] The likelihood are the y-axis values (i.e. density) for fixed data points with distributions that can move, that is:\n\\[L(distribution\\ |\\ data)\\] The likelihood is the height of the density function. Figure 7 depicts two likelihood for two observations. The mean of each distribution is equal to \\(\\beta_{wt} X + e\\) and the intercept (also known as the error term) defines the standard deviation of the distribution.\n\n\n\n\n\nFigure 7. Likelihood of a car having the observed mpg given the model parameters for two observations.\n\n\n\n\nWe can then calculate the likelihood for each observation in our data. Unlike OLS, we now want to maximize the sum of these values. Also, we are going to use the log of the likelihood so we can add them instead of multiplying. We can now define our log likelihood function:\n\n\nShow the code\nloglikelihood &lt;- function(parameters, predictor, outcome) {\n    a &lt;- parameters[1]     # intercept\n    b &lt;- parameters[2]     # slope / beta coefficient\n    sigma &lt;- parameters[3] # error\n    ll.vec &lt;- dnorm(outcome, a + b * predictor, sigma, log = TRUE)\n    return(sum(ll.vec))\n}\n\n\nNote that we have to estimate a third parameter, sigma, which is the error term and defines the standard deviation for the normal distribution for estimating the likelihood. This is connected to the distribution of the residuals as we will see later. We can now calculate the log-likelihood for any combination of parameters.\n\n\nShow the code\nloglikelihood(c(37, -5, sd(mtcars$mpg)),\n              predictor = mtcars$wt,\n              outcome = mtcars$mpg)\n\n\n[1] -91.06374",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "mle.html#maximum-likelihood-estimation",
    "href": "mle.html#maximum-likelihood-estimation",
    "title": "8  Maximum Likelihood Estimation",
    "section": "8.4 Maximum Likelihood Estimation",
    "text": "8.4 Maximum Likelihood Estimation\nWe can now use the optim_save function to find the parameters that maximize the log-likelihood. Note two important parameter changes:\n\nWe are specifying the lower parameter so that the algorithm will not try negative values for sigma since the variance cannot be negative.\nThe value for the control parameter indicates that we wish to maximize the values instead of minimizing (which is the default).\n\n\n\nShow the code\noptim.ll &lt;- optim_save(\n    runif(3),                     # Random initial values\n    loglikelihood,                # Log-likelihood function\n    lower = c(-Inf, -Inf, 1.e-5), # The lower bounds for the values, note sigma (error), cannot be negative\n    method = \"L-BFGS-B\",\n    control = list(fnscale = -1), # Indicates that the maximum is desired rather than the minimum\n    predictor = mtcars$wt,\n    outcome = mtcars$mpg\n)\n\n\nWe can get our results and compare them to the results of the lm function and find that they match to at least four decimal places.\n\n\nShow the code\noptim.ll$par[1:2]\n\n\n[1] 37.285114 -5.344468\n\n\nShow the code\nlm.out$coefficients\n\n\n(Intercept)          wt \n  37.285126   -5.344472 \n\n\nFigure 8 shows the estimated regression line for each iteration of the optimization procedure (on the left; OLS regression line in blue; MLE regression line in black) with the estimated parameters and log-likelihood for all iterations on the left.\nFigure 9 superimposes the normal distribution from which the log-likelihood is determined. The distribution is centered on \\(\\hat{y}\\). The height of the distribution (i.e. density) at \\(y\\) is the likelihood. We take the log of this value to get the log-likelihood. These log-likelihoods are calculated for each observation and summed. Maximum likelihood estimation is attempting to find the parameters (i.e. slope and intercept) that maximizes the log-likelihood.\n\n\nShow the code\nVisualStats::plot_likelihood(x = mtcars$wt, \n                           y = mtcars$mpg,\n                           pt = 2,\n                           intercept = optim.ll$par[1],\n                           slope = optim.ll$par[2],\n                           sigma = optim.ll$par[3])\n\n\n\n\n\nFigure 9. Likelihood for one observeration superimposed on scatter plot.\n\n\n\n\nFigure 10 depicts the likelihoods for the first 16 observations.\n\n\nShow the code\ndf &lt;- optim.ll$iterations_df\nnames(df) &lt;- c('Intercept', 'Slope', 'Sigma', 'LogLikelihood', 'Iteration')\ntmp &lt;- df |&gt; dplyr::filter(Iteration == nrow(df))\nplots &lt;- list()\nnplots &lt;- 16 #nrow(mtcars)\nfor(i in 1:min(nplots, nrow(mtcars))) {\n    a &lt;- tmp[1,]$Intercept\n    b &lt;- tmp[1,]$Slope\n    sigma &lt;- tmp[1,]$Sigma\n    predictor &lt;- mtcars$wt[i]\n    predicted.out &lt;- a + b * predictor\n    outcome &lt;- mtcars$mpg[i]\n    d &lt;- dnorm(outcome, predicted.out, sigma)\n    plots[[i]] &lt;- ggplot() +\n        stat_function(fun = dnorm,\n                      n = 101,\n                      args = list(mean = predicted.out, sd = sigma)) +\n        annotate(geom = 'segment', x = outcome, y = 0, xend = outcome, yend = d, color = 'red') +\n        annotate(geom = 'point', x = outcome, y = d, color = 'red', size = 2) +\n        xlim(c(min(mtcars$mpg, predicted.out - 3 * sigma),\n               max(mtcars$mpg, predicted.out + 3 * sigma))) +\n        ylim(c(0, .2)) +\n        ylab('') + xlab(row.names(mtcars)[i])\n}\nplot_grid(plotlist = plots)\n\n\n\n\n\nFigure 10. Likelihoods of the first 16 observations for the final parameter estimates.\n\n\n\n\nWith MLE we need to estimate what is often referred to as the error term, or as we saw above is the standard deviation of the normal distribution from which we are estimating the likelihood from. In Figure 9 notice that the normal distribution id drawn vertically. This is because the likelihood is estimated from the error, or the residuals. In OLS we often report the root-mean-square deviation (RMSD, or root-mean-square error, RMSE). The RMSD is the standard deviation of the residuals:\n\\[RMSD\\  =\\  \\sqrt{\\frac{\\sum^{N}_{i=1} (x_{i}-\\hat{x_{i}} )^{2}}{N} }\\] Where \\(i\\) is the observation, \\(x_i\\) is the observed value, \\(\\hat{x_i}\\) is the estimated (predicted) value, and \\(N\\) is the sample size. Below, we see that the numerical optimizer matches the RMSD within a rounding error.\n\n\nShow the code\noptim.ll$par[3]\n\n\n[1] 2.949164\n\n\nShow the code\nsqrt(sum(resid(lm.out)^2) / nrow(mtcars))\n\n\n[1] 2.949163",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "mle.html#generalized-linear-models",
    "href": "mle.html#generalized-linear-models",
    "title": "8  Maximum Likelihood Estimation",
    "section": "8.5 Generalized Linear Models",
    "text": "8.5 Generalized Linear Models\nGeneralized linear models (GLM) are a generalization of OLS that allows for the response variables (i.e. dependent variables) to have an error distribution that is not distributed normally. All generalized linear models have the following three characteristics:\n\nA probability distribution describing the outcome variable .\nA linear model\n\\(\\eta = \\beta_0+\\beta_1 X_1 + \\cdots + \\beta_n X_n\\).\nA link function that relates the linear model to the parameter of the outcome distribution\n\\(g(p) = \\eta\\) or \\(p = g^{-1}(\\eta)\\).\n\nWe can estimate GLMs using MLE as described above. What will change is the log-likelihood function.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "mle.html#logistic-regression",
    "href": "mle.html#logistic-regression",
    "title": "8  Maximum Likelihood Estimation",
    "section": "8.6 Logistic Regression",
    "text": "8.6 Logistic Regression\nLogistic regression is a GLM used to model a binary categorical variable using numerical and categorical predictors. We assume a binomial distribution produced the outcome variable and we therefore want to model p the probability of success for a given set of predictors. Instead of fitting a line (or a plane for two predictors, etc. for higher dimensions) we wish to fit the data to the logistic function which is defined as:\n\\[ \\sigma \\left( t \\right) =\\frac { { e }^{ t } }{ { e }^{ t }+1 } =\\frac { 1 }{ 1+{ e }^{ -t } }  \\]\n\n\nShow the code\nlogistic &lt;- function(t) { \n    return(1 / (1 + exp(-t))) \n}\nggplot() +\n    stat_function(fun = logistic, n = 101) +\n    xlim(-4, 4) + xlab('x') +\n    theme_vs()\n\n\n\n\n\nFigure 11. Logistic curve\n\n\n\n\nTo finish specifying the Logistic model we just need to establish a reasonable link function that connects \\(\\eta\\) to \\(p\\). There are a variety of options but the most commonly used is the logit function which is specified as:\n\\[logit(p) = \\log\\left(\\frac{p}{1-p}\\right),\\text{ for $0\\le p \\le 1$}\\]\nWe can specify t as a linear combination of our predictors (independent variables).\n\\[ t = \\beta_0 + \\beta_1 x \\]\nThe logistic function can now be rewritten as:\n\\[ F\\left( x \\right) =\\frac { 1 }{ 1+{ e }^{ -\\left( { \\beta  }_{ 0 }+\\beta _{ 1 }x \\right)  } } \\]\nConsider the following data set where we wish to predict whether a student will pass an exam based upon the number of hours they studied.2.\n\n\nShow the code\nstudy &lt;- data.frame(\n    Hours=c(0.50,0.75,1.00,1.25,1.50,1.75,1.75,2.00,2.25,2.50,2.75,3.00,\n            3.25,3.50,4.00,4.25,4.50,4.75,5.00,5.50),\n    Pass=c(0,0,0,0,0,0,1,0,1,0,1,0,1,0,1,1,1,1,1,1)\n)\n\n\n\n\n\n\n\nFigure 12. Boxplot of hours studied by passing.\n\n\n\n\nFirst, we need to define logit function and the log-likelihood function that will be used by the optim function. Instead of using the normal distribution as above (using the dnorm function), we are using a binomial distribution and the logit to link the linear combination of predictors.\n\n\nShow the code\nlogit &lt;- function(x, beta0, beta1) {\n    return( 1 / (1 + exp(-beta0 - beta1 * x)) )\n}\nloglikelihood.binomial &lt;- function(parameters, predictor, outcome) {\n    a &lt;- parameters[1] # Intercept\n    b &lt;- parameters[2] # beta coefficient\n    p &lt;- logit(predictor, a, b)\n    ll &lt;- sum( outcome * log(p) + (1 - outcome) * log(1 - p))\n    return(ll)\n}\n\n\nNow we can call the optim function and get the final parameter estimates.\n\n\nShow the code\noptim.binomial &lt;- optim_save(\n    c(0, 1), # Initial values\n    loglikelihood.binomial,\n    method = \"L-BFGS-B\",\n    control = list(fnscale = -1),\n    predictor = study$Hours,\n    outcome = study$Pass\n)\n\n\nIn R, the glm (short for generalized linear models) function implements logistic regression when the family = binomial(link = 'logit') parameter is set. See ?glm for other families of models to estimate models with other underlying distributions. We can see that our estimate matches the results of glm to a rounding error.\n\n\nShow the code\noptim.binomial$par\n\n\n[1] -4.077575  1.504624\n\n\nShow the code\nlr.out &lt;- glm(Pass ~ Hours, data = study, family = binomial(link = 'logit'))\nlr.out$coefficients\n\n\n(Intercept)       Hours \n  -4.077713    1.504645 \n\n\n\n\nShow the code\n# Redefine the logistic function to include parameter estimates\nlogistic &lt;- function(x, beta0, beta1) {\n    return(1 / (1 + exp(-1 * (beta0 + beta1 * x)) ))\n}\n\nbeta0 &lt;- optim.binomial$par[1]\nbeta1 &lt;- optim.binomial$par[2]\n\nggplot(study, aes(x = Hours, y = Pass)) +\n    geom_point(aes(color = logistic(Hours, beta0, beta1) &gt; 0.5)) +\n    stat_function(fun = logistic, n = 101, \n                  args = list(beta0 = beta0, beta1 = beta1) ) +\n    scale_color_hue('Predicted Pass &gt; 0.5') +\n    theme(legend.position = 'inside', legend.position.inside = c(0.85, 0.15)) +\n    theme_vs()\n\n\n\n\n\n\n\n\n\nLet’s explore the process of the numeric optimizer. For this model, it took 70 iterations to converge to resulting parameters.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \nThis Shiny application can be run locally using the VisualStats::mle_shiny() function.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "mle.html#footnotes",
    "href": "mle.html#footnotes",
    "title": "8  Maximum Likelihood Estimation",
    "section": "",
    "text": "This function is available in the VisualStats package.↩︎\nThese data were retrived from the Wikipedia article on logistic regresion↩︎",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "multiple_regression.html",
    "href": "multiple_regression.html",
    "title": "9  Multiple Regression",
    "section": "",
    "text": "9.1 Interaction Effects\nNow that we have defined the relationship between two variables using ordinary least squares regression, we want to extend that framework to include omre variables. Multiple regression is the procedure used to predict an outcome (i.e. dependent variable) from two or more independent variables. We are going to use the depression data set which includes baseline measures from 200 individuals who participated in psychological intervention (Boswell, n.d.). We can visually represent these data using a 3d scatter plot where our dependent variable is on the z-axis and independent variables are on the x and y-axes.\nAlgebraically we represent this model as:\n\\[Y_{depression} = B_0 + B_{anxiety} X_{anxiety} + B_{affect} X_{affect}\\]\nWe now have two slopes (coefficients) to estimate In the OLS regression chapter we defined the slope of the line as the product of the correlation and the ratio of the standard deviations. We might be tempted to estimate the two slopes using:\n\\[ B_{anxiety} = r_{depression,anxiety} \\frac{S_{depression}}{S_{anxiety}} \\]\n\\[ B_{affect} = r_{depression,affect} \\frac{S_{depression}}{S_{affect}} \\]\nHowever, this would be incorrect because anxiety and affect are correlated.\nAlthough it is possible to estimate the slopes using a closed form solution, it requires knowledge of matrix algebra which is beyond the scope of this book. Instead, to explore multiple regression visually, we will use maximum likelihood estimation. Our goal, like OLS, is to minimize the sum of squared residuals. The following function generalizes the calculation of the sum of squared residuals introduced in the maximum likelihood estimation for an arbitrary number of independent variables.\nWe can then use the optim_save function to find the combination of slopes and intercept that minimizes the sum of squared residuals.\nThe regression results can then be written algebrically as:\n\\[ Y_{depression} = 5.74 + 0.58 X_{anxiety} + -0.21 X_{affect} \\]\nThe figure below adds the plane estimated to the 3d scatter plot. The vertical lines going from each point to the plane represent the residuals.\nMultiple regression uses the same lm function to estimate the model, we simply add our additional independent variables.\nShow the code\nmultiple_regression_vis(y = depression$depression,\n                        x1 = depression$anxiety,\n                        x2 = depression$affect,\n                        y_lab = 'Depression',\n                        x1_lab = 'Anxiety',\n                        x2_lab = 'Affect',\n                        plot_regression = TRUE,\n                        plot_residuals = FALSE,\n                        interaction = TRUE)\nShow the code\nlm_out_2 &lt;- lm(depression ~ anxiety * affect, data = depression)\nsummary(lm_out_2)\n\n\n\nCall:\nlm(formula = depression ~ anxiety * affect, data = depression)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-8.118 -2.487 -0.265  2.315 10.027 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -8.41373    3.53037   -2.38    0.018 *  \nanxiety         1.87034    0.29608    6.32  1.8e-09 ***\naffect          0.24870    0.10933    2.27    0.024 *  \nanxiety:affect -0.04303    0.00958   -4.49  1.2e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.5 on 196 degrees of freedom\nMultiple R-squared:  0.466, Adjusted R-squared:  0.458 \nF-statistic:   57 on 3 and 196 DF,  p-value: &lt;2e-16",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "r_squared.html",
    "href": "r_squared.html",
    "title": "10  R-Squared",
    "section": "",
    "text": "The coefficient of determination, often referred to as \\(R^2\\), is an important measure of model fit in statistics and data science when the dependent variable is quantitative. First introduced by Write (Write1921?), \\(R^2\\) is the proportion of variance in the dependent variable explained by the independent variable(s).\nTo begin, we must first define variance. Broadly, variance is a measure of spread around the mean. Most textbooks provide the following equation for variance:\nTotal sum of squares:\n\\[ SS_{total} = \\Sigma{(x_i - \\bar{x})^2} \\]\nRegression sum of squares:\n\\[ SS_{regression} = \\Sigma{(\\hat{y_i} - \\bar{y})^2} \\]\nError sum of squares:\n\\[ SS_{error} = \\Sigma{(y_i - \\hat{y_i})^2} \\]\nThinking of variance as a measure of variation is generally approachable for students, and interestingly, Write (1921) used the word variation instead of variance in his seminal paper. The key for students is in the numerator, where each observation is subtracted from the mean (i.e. \\(x_i - \\bar{x}\\)).\n\n\n\n\n\nPlot representing variance.\n\n\n\n\nIt turns out there are several ways of calculating \\(R^2\\), many provide equivelent solutions. Most introductions rely on algebra and complicated formulas. This paper provides an approach to understanding \\(R^2\\) using visualizations.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScatter plots with residuals (black lines), resisuals squared (grey squares), and mean square error (orange; average area of the residuals squared).\n\n\n\n\n\n\n\n\n\nScatter plots with total variance (grey square) with error variance (left; orange) and regression variance (left; blue)\n\n\n\n\n\n\n\n\n\nScatter plot with variance components.\n\n\n\n\n\n10.0.1 Example 2\n\n\nShow the code\nlibrary(VisualStats)\nset.seed(42)\ndf &lt;- VisualStats::simulate(n = 100, r_squared = .7)\nformu &lt;- y ~ x1 + x2\nlm(formu, df) |&gt; summary()\n\n\n\nCall:\nlm(formula = formu, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.0818 -1.5623 -0.1948  1.5037  5.9495 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   5.0042     0.2405   20.80  &lt; 2e-16 ***\nx1            2.6608     0.2310   11.52  &lt; 2e-16 ***\nx2           -1.7987     0.2661   -6.76 1.04e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.393 on 97 degrees of freedom\nMultiple R-squared:  0.6416,    Adjusted R-squared:  0.6342 \nF-statistic: 86.82 on 2 and 97 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nShow the code\nr_squared_vis(df, formu,\n              plot_total_variance = FALSE,\n              plot_error_variance = FALSE,\n              plot_regression_variance = FALSE,\n              plot_all_variances = FALSE,\n              plot_residuals_squared = FALSE,\n              plot_residuals = FALSE)\n\n\n\n\n\nScatter plot of observed values versus predicted values.\n\n\n\n\n\n\nShow the code\np1 &lt;- r_squared_vis(df, formu,\n              plot_total_variance = TRUE,\n              plot_error_variance = FALSE,\n              plot_regression_variance = FALSE,\n              plot_all_variances = FALSE,\n              plot_residuals_squared = FALSE,\n              plot_residuals = FALSE) + \n    ggplot2::ylim(c(-20,20)) +\n    ggplot2::xlim(c(-20,20)) + ggplot2::ggtitle('')\np2 &lt;- variance_vis(df$y, \n                   sample_variance_col = '#999999', \n                   plot_sample_variance = TRUE,\n                   plot_population_variance = FALSE,\n                   variance_position = 'middle',\n                   point_size = 1) + \n    ggplot2::ylim(c(0,20))\ncowplot::plot_grid(p1, p2)\n\n\n\n\n\nTotal Variance\n\n\n\n\n\n\nShow the code\nr_squared_vis(df, formu,\n              plot_total_variance = FALSE,\n              plot_error_variance = FALSE,\n              plot_regression_variance = FALSE,\n              plot_all_variances = FALSE,\n              plot_residuals_squared = TRUE,\n              plot_residuals = TRUE)\n\n\n\n\n\nResisuals and squared residuals\n\n\n\n\n\n\nShow the code\nr_squared_vis(df, formu,\n              plot_total_variance = FALSE,\n              plot_error_variance = TRUE,\n              plot_regression_variance = FALSE,\n              plot_all_variances = FALSE,\n              plot_residuals_squared = TRUE,\n              plot_residuals = TRUE)\n\n\n\n\n\nResisuals and squared residuals\n\n\n\n\n\n\nShow the code\nr_squared_vis(df, formu,\n              plot_total_variance = TRUE,\n              plot_error_variance = TRUE,\n              plot_regression_variance = FALSE,\n              plot_all_variances = FALSE,\n              plot_residuals_squared = FALSE,\n              plot_residuals = FALSE)\n\n\n\n\n\nResisuals and squared residuals",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>R-Squared</span>"
    ]
  },
  {
    "objectID": "logistic_regression.html",
    "href": "logistic_regression.html",
    "title": "11  Logistic Regression",
    "section": "",
    "text": "11.1 Linear Regression\nThis document outlines the various ways we could model dichotomous outcomes and why the log likelihood is the preferred approach. We will work through a small data set from Wikipedia that examines the relationship between the number of hours studied and passing an exam.\nWe will first consider simple linear regression.\nShow the code\nlinear_regression &lt;- lm(Pass ~ Hours, data = study)\nlinear_regression\n\n\n\nCall:\nlm(formula = Pass ~ Hours, data = study)\n\nCoefficients:\n(Intercept)        Hours  \n    -0.1539       0.2346\nThe resulting linear regression line is presented below.\nShow the code\nggplot(study, aes(x = Hours, y = Pass)) + \n    geom_point(aes(color = factor(Pass))) +\n    geom_abline(slope = linear_regression$coefficients[2],\n                intercept = linear_regression$coefficients[1]) +\n    scale_color_brewer('Pass', type = 'qual', palette = 6)\nThe \\(R^2\\) for this model is 0.48 which, on the surface, does not appear to be too bad. However, examining the residual plot shows that the homoscedasticity assumption of linear regression is clearly violated.\nShow the code\nstudy$linear_resid &lt;- resid(linear_regression)\nggplot(study, aes(x = Hours, y = linear_resid)) + geom_point()\nAs we proceed, we will want to estimate the parameters using a numeric optimizer. The optim function in R implements a number of routines that either minimize or maximize parameter estimates based upon the output of a function. For ordinary least squares, we wish to minimize the sum of squared residuals (RSS). The function below calculates the RSS for a given set of parameters (i.e. slope and intercept).\nShow the code\nols_linear &lt;- function(parameters, predictor, outcome) {\n    a &lt;- parameters[1] # Intercept\n    b &lt;- parameters[2] # beta coefficient\n    predicted &lt;- a + b * predictor\n    residuals &lt;- outcome - predicted\n    ss &lt;- sum(residuals^2)\n    return(ss)\n}\nTo get the parameter estimates, we call the optim function with the ols_linear function defined above.\nShow the code\noptim_ols_linear &lt;- optim(\n    c(0, 1), # Initial values\n    ols_linear,\n    method = \"L-BFGS-B\",\n    predictor = study$Hours,\n    outcome = study$Pass\n)\noptim_ols_linear$par\n\n\n[1] -0.1539353  0.2345956\nWe see that the parameter estimates are the same as the lm function to several decimal places. For the rest of this document, we will use this approach to estimate parameter estimates and simply modify the metric we wish to optimize.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "logistic_regression.html#minimize-residuals-for-the-logistic-function",
    "href": "logistic_regression.html#minimize-residuals-for-the-logistic-function",
    "title": "11  Logistic Regression",
    "section": "11.2 Minimize Residuals for the Logistic Function",
    "text": "11.2 Minimize Residuals for the Logistic Function\nSince the relationship between hours studies and passing is not linear, we can try a non-linear method. The logistic curve is an S shape function which, on the surface, would appear to be a better approach.\n\n\nShow the code\nlogit &lt;- function(x, beta0, beta1) {\n    return( 1 / (1 + exp(-beta0 - beta1 * x)) )\n}\n\nggplot() + \n    stat_function(fun = logit, args = list(beta0 = 0, beta1 = 1)) +\n    xlim(-5, 5)\n\n\n\n\n\n\n\n\n\nOur first approach will be to minimize the sum of the absolute value of the residuals.\n\n\nShow the code\nmin_abs_resid &lt;- function(parameters, predictor, outcome) {\n    a &lt;- parameters[1] # Intercept\n    b &lt;- parameters[2] # beta coefficient\n    p &lt;- logit(predictor, a, b)\n    resid &lt;- outcome - p\n    return(sum(abs(resid)))\n}\n\n\n\n\nShow the code\noptim_min_abs_resid &lt;- optim(\n    c(0, 1), # Initial values\n    min_abs_resid,\n    method = \"L-BFGS-B\",\n    predictor = study$Hours,\n    outcome = study$Pass\n)\noptim_min_abs_resid$par\n\n\n[1] -253.21473   67.52317\n\n\n\n\nShow the code\nggplot(data = study, aes(x = Hours, y = Pass)) + \n    geom_point(aes(color = factor(Pass))) +\n    geom_function(fun = logit, args = list(beta0 = optim_min_abs_resid$par[1], \n                                           beta1 = optim_min_abs_resid$par[2])) +\n    scale_color_brewer('Pass', type = 'qual', palette = 6)\n\n\n\n\n\n\n\n\n\nExamining the resulting model shows that this approach is an overfit of the data. Next, let’s try minimizing the RSS using the logistic function.\n\n\nShow the code\nols_logistic &lt;- function(parameters, predictor, outcome) {\n    a &lt;- parameters[1] # Intercept\n    b &lt;- parameters[2] # beta coefficient\n    p &lt;- logit(predictor, a, b)\n    resid &lt;- outcome - p\n    return(sum(resid^2))\n}\n\n\n\n\nShow the code\noptim_ols_logistic &lt;- optim(\n    c(0, 1), # Initial values\n    ols_logistic,\n    method = \"L-BFGS-B\",\n    predictor = study$Hours,\n    outcome = study$Pass\n    \n)\noptim_ols_logistic$par\n\n\n[1] -3.296428  1.199072\n\n\n\n\nShow the code\nggplot(data = study, aes(x = Hours, y = Pass)) + \n    geom_point(aes(color = factor(Pass))) +\n    geom_function(fun = logit, args = list(beta0 = optim_ols_logistic$par[1], \n                                           beta1 = optim_ols_logistic$par[2])) +\n    scale_color_brewer('Pass', type = 'qual', palette = 6)\n\n\n\n\n\n\n\n\n\nThis certainly appears to do a better job of fitting the data. Let’s examine the residuals.\n\n\nShow the code\nstudy$predict_ols_logistic &lt;- logit(study$Hours,\n                                    beta0 = optim_ols_logistic$par[1],\n                                    beta1 = optim_ols_logistic$par[2])\nstudy$resid_ols_logistc &lt;- study$Pass - study$predict_ols_logistic\nggplot(study, aes(x = Hours, y = resid_ols_logistc)) + geom_point()\n\n\n\n\n\n\n\n\n\nAnd like above, the homoscedasticity assumption is violated.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "logistic_regression.html#maximize-log-likelihood",
    "href": "logistic_regression.html#maximize-log-likelihood",
    "title": "11  Logistic Regression",
    "section": "11.3 Maximize Log Likelihood",
    "text": "11.3 Maximize Log Likelihood\nThe results of logit function can be interpreted as the likelihood of the outcome given the independent (predictor) variable. Instead of minimizing the residuals, defined above as the difference between the likelihood and observed outcome, we can minimize the log of the likelihood. Note that in the function below, we are using the log of 1 - likelihood (we’ll discuss why below).\n\n\nShow the code\nloglikelihood.binomial &lt;- function(parameters, predictor, outcome) {\n    a &lt;- parameters[1] # Intercept\n    b &lt;- parameters[2] # beta coefficient\n    p &lt;- logit(predictor, a, b)\n    ll &lt;- sum( outcome * log(p) + (1 - outcome) * log(1 - p))\n    return(ll)\n}\n\n\n\n\nShow the code\noptim_binomial &lt;- optim_save(\n    c(0, 1), # Initial values\n    loglikelihood.binomial,\n    method = \"L-BFGS-B\",\n    control = list(fnscale = -1),\n    predictor = study$Hours,\n    outcome = study$Pass\n)\noptim_binomial$par\n\n\n[1] -4.077575  1.504624\n\n\nWe see that the fit is slightly different here (the OLS approach is included as a dash line).\n\n\nShow the code\nggplot(data = study, aes(x = Hours, y = Pass)) + \n    geom_point(aes(color = factor(Pass))) +\n    geom_function(fun = logit, args = list(beta0 = optim_ols_logistic$par[1], \n                                           beta1 = optim_ols_logistic$par[2]),\n                  linetype = 2) +\n    geom_function(fun = logit, args = list(beta0 = optim_binomial$par[1], \n                                           beta1 = optim_binomial$par[2])) +\n    scale_color_brewer('Pass', type = 'qual', palette = 6)\n\n\n\n\n\n\n\n\n\nThe log Function. Since we know our outcomes are either zero or one, and hence bounded by zero and one, then we are only considering values of log(x) between zero and one. The plot below shows that log(x) for all \\(0 \\leq 1x \\leq 1\\) is negative, going asymptotically to \\(-\\infty\\) as x approaches zero.\n\n\nShow the code\nggplot() + geom_function(fun = log) + xlim(0, 1)\n\n\n\n\n\n\n\n\n\nSince \\(log(1) = 0\\), we want to reverse the likelihood, that is, we will take the \\(log(1 - likelihood)\\) so that smaller errors result in larger log-likelihood values.\nTo put this all together, we will calculate the likelihood (i.e. predicted value using the logistic function), and the log-likelihood (what we maximized). The plot shows the observed values (as triangles), predicted values (as squares), and the log-likelihood (as circles). The lines represent the values we are taking the log of. That is, if Pass == 0, then we take the log of 1 - likelihood; conversely if Pass == 1 then we take the log of the likelihood. The circles below the line y = 0 are the log values. As can be seen, since they are all negative we wish to maximize the sum of their values to achieve the best fit.\n\n\nShow the code\nstudy$likelihood &lt;- logit(study$Hours,                                 \n                          beta0 = optim_binomial$par[1], \n                          beta1 = optim_binomial$par[2])\nstudy$log_likelihood &lt;- study$Pass * log(study$likelihood) +         # If Pass == 1\n                        (1 - study$Pass) * log(1 - study$likelihood) # If Pass == 0\n\n\n\n\nShow the code\nggplot(data = study, aes(x = Hours, y = Pass)) + \n    geom_smooth(method = 'glm', formula = y ~ x,\n                method.args = list(family=binomial(link='logit')), se = FALSE, alpha = 0.2) +\n    geom_hline(yintercept = 0) +\n    geom_function(fun = logit, color = 'grey50', size = 1,\n                  args = list(beta0 = optim_binomial$par[1], beta1 = optim_binomial$par[2])) +\n    geom_segment(aes(xend = Hours, y = 1 - Pass, yend = likelihood, color = factor(Pass)), alpha = 0.5) +\n    geom_point(aes(y = likelihood, color = factor(Pass), shape = 'Predicted'), size = 3) +\n    geom_point(aes(color = factor(Pass), shape = 'Observed'), size = 3) +\n    geom_point(aes(y = log_likelihood, color = factor(Pass), shape = 'Log Likelihood'), size = 3) +\n    scale_color_brewer('Pass', type = 'qual', palette = 6)",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "logistic_regression.html#assumptions",
    "href": "logistic_regression.html#assumptions",
    "title": "11  Logistic Regression",
    "section": "11.4 Assumptions",
    "text": "11.4 Assumptions\nAlthough maximizing the log-likelihood provides a result similar to minimizing the sum of squared residuals using the logistic function, the log-likelihood doesn’t rely on the assumptions of residuals OLS does. Namely:\n\nThere is no assumption of linearity between the dependent and independent variables.\nHomoscedasticity (constant variance) is not for logistic regression (but is for linear regression).\nThe residuals do not have to be normally distributed.\n\nThere is an assumption of linearity between the independent variable(s) and the log-odds.\n\n\nShow the code\nlr.out &lt;- glm(Pass ~ Hours, data = study, family = binomial(link='logit'))\nplot_linear_assumption_check(lr.out, n_groups = 5)\n\n\n\n\n\n\n\n\n\nThe Box-Tidewell test can be used to check for linearity between the predictor(s) and the logit. This is implemented in the car package in R. We are looking for a non-significant value here.\n\n\nShow the code\nstudy$logodds &lt;- lr.out$linear.predictors\ncar::boxTidwell(logodds ~ Hours, data = study) # Looking for a non-significant p-value\n\n\n MLE of lambda Score Statistic (t) Pr(&gt;|t|)\n             1              0.9682   0.3465\n\niterations =  0 \n\n\nThere are a number of assumptions logistic regression shares with OLS, namely:\n\nOutliers - check to see if there are any strongly influential outliers.\nNo multicollinearity\nIndependence of observations\nSufficiently large sample size",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "loess.html",
    "href": "loess.html",
    "title": "12  Loess Regression",
    "section": "",
    "text": "LOESS (locally estimated scatterplot smoothing) provides a method for visualizing the relationship between variables when classical linear and non-linear least squares regression may not be the most appropriate. The non-parametric technique explores the relationship between variables using local regressions using only regions of the explanatory variable (IV).\nThis shiny app permits visualization of those local regressions along the whole of the X variable space. The way that the loess curve is calculated is to use the predicted value in the response dimension (DV) for the chosen X value based on the local regression. These predicted points are visualized with the orange dot on the plot and the curve can be seen to be “drawn” when using animation of the centering slider.\nThe ‘loess’ function in R provides the capability for either first or second degree polynomial specification for the loess fit (linear or quadratic) and this shiny app provides that same choice along with the “span” specification which affects the smoothing outcome. Center and span work by locating the local regressions and determining the X axis range employed.\nThe loess algorithm uses in R and in this shiny app follow an approach developed by Cleveland (1979) and which was apparently also in use in some fields as the Savitsky-Golay filter (Savitzky and Golay 1964). It weights data points closer to the center of the localized regression more heavily than those more distanced.\nThis app does not permit choice of smoothing family. It uses the default Gaussian kernel (least squares approach) of the loess function in R.\nThe weights for each x are calculated using:\n\\[\\left( 1 - \\left(\\frac{dist}{max(dist)}\\right)^3 \\right)^3\\]\nwhen \\(\\alpha &lt; 1\\) (i.e. span).\n\n\nShow the code\ndata('faithful')\nloess_vis(formula = eruptions ~ waiting,\n          data = faithful)\n\n\n\n\n\n\n\n\n\n \nVisualization of Loess regression. This app is a visual/conceptual demonstration of the elements of a Loess Plot.The orange point plots the predicted value from an X axis value midway along a local regression shown by the local (green) regression fit. The center of this locally weighted regression and its X axis span can be controlled by the CENTER slider and SPAN numeric entry box. Best use of the app would use the animation control for the CENTER slider and the checkbox for actively drawing the Loess fit. The full loess fit can also be displayed with the second checkbox. The local regression can be specifed as a linear or quadratic fit with the DEGREE dropdown.\n\n\n\n\nCleveland, W. S. 1979. “Robust Locally Weighted Regression and Smoothing Scatterplots.” Journal of the American Statistical Association 74.\n\n\nSavitzky, A., and M. J. E. Golay. 1964. “Smoothing and Differentiation of Data by Simplified Least Squares Procedures.” Analytical Chemistry 36.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Loess Regression</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Allaire, JJ, and Christophe Dervieux. 2024. Quarto: R Interface to\n’Quarto’ Markdown Publishing System. https://CRAN.R-project.org/package=quarto.\n\n\nAristotle. 1929. Physics. Translated by P. H. Wicksteed and F.\nM. Cornford. New York: G. P. Putnam.\n\n\nChang, Winston, Joe Cheng, JJ Allaire, Carson Sievert, Barret Schloerke,\nYihui Xie, Jeff Allen, Jonathan McPherson, Alan Dipert, and Barbara\nBorges. 2023. Shiny: Web Application Framework for r. https://CRAN.R-project.org/package=shiny.\n\n\nCleveland, W. S. 1979. “Robust Locally Weighted Regression and\nSmoothing Scatterplots.” Journal of the American Statistical\nAssociation 74.\n\n\nPruzek, Robert, and James Helmreich. 2010. “Elemental Graphics for\nAnalysis of Variance Using the r Package granova.”\n\n\nSavitzky, A., and M. J. E. Golay. 1964. “Smoothing and\nDifferentiation of Data by Simplified Least Squares Procedures.”\nAnalytical Chemistry 36.\n\n\nWickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data\nAnalysis. Springer-Verlag New York. https://ggplot2.tidyverse.org.\n\n\nWilkinson, Leland. 2005. The Grammar of Graphics (Statistics and\nComputing). Berlin, Heidelberg: Springer-Verlag.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "calculus.html",
    "href": "calculus.html",
    "title": "Appendix A — Calculus",
    "section": "",
    "text": "A.1 Limits\nAlthough calculus is not required prerequisite for learning statistics, having a basic understanding of the three core concepts from calculus can be helpful. This appendix provides some visualizations and Shiny applications to provide a crash course in calculus, specifically related to limits (useful for the Central Limit Theorem), derivatives (use for the ordinary least squares), and integrals (useful for probability distributions, p-values, and confidence intervals).\nIsaac Newton and Gottfried Wilhelm Leibniz are credited for developing calculus independently of each other in the 17th century. While Newton developed calculus to solve problems in physics, Leibniz developed much of the notation we use today. Although calculus is now a significant part of mathematics, Newton in particular, developed calculus to solve problems in physics. In particular Newton wanted to understand the relationship between acceleration, velocity, and distance. Figure A.1 depicts acceleration (assuming constant acceleration of 1.0 m/s), velocity, and distance with time on the x-axis. Starting with acceleration, we can see that it is flat. With a constant acceleration of 1.0 m/s, we are stationary at time point zero, but for every second velocity increases by 1.0 m/s such that velocity is 2.0 m/s at time point two, 3.0 m/s at time point three, and etc. Distance is a bit trickier to calculate since velocity is constantly increasing. The distance traveled from time one to two is larger than from time zero to one since velocity has doubled. It turns out that with a linear increase in velocity results in a quadratic increase in distance.\nCalculus provides the tools to define the relationship between these functions. In this particular example letting t stand for time, we have:\n\\[f_{acceleration}(t) = 1.0 \\tag{A.1}\\] \\[f_{velocity}(t) = f_{acceleration}(t) * t \\tag{A.2}\\] \\[f_{distance}(t) = f_{velocity}(t) * t + \\frac{(f_{acceleration}(t) * t)^2}{2} \\tag{A.3}\\]\nTo go from \\(f_{distance}(t)\\) to \\(f_{acceleration}(t)\\) you take the derivative, going the other direction you integrate. The fundamental theorem of calculus states that differentiation and integration are inverse operations. This development has had a profound impact on mathematics, calculus, as well as many other fields. In the following sections we will describe in more detail through visualizations the parts of calculus that are most relevant to statistics.\nAncient Greek philosopher Zeno of Elea posed a number of paradoxical scenarios. Aristotle (1929) describe Zeno’s paradox of dichotomy as:\nStated another way, consider standing in front of a wall 1 meter away. You can walk half the distance to get to 0.5 meters away, Then again to arrive at 0.25 meters away. Given this scenario where you can constantly halve your distance to the wall, Zeno’s paradox would state that you would never reach the wall. Figure A.2 depicts the first 10 iterations. As we can see very quickly we get very close to zero, though never to zero, such that the lines are indistinguishable from on another. However, we know we do actually reach the wall. In calculus we call this the limit. For Zeno’s paradox of dichotomy, we would say the limit as the number of halving the distance approaches infinity is zero.\nShow the code\npath &lt;- c(1, rep(NA, 10))\nfor(i in 2:length(path)) {\n    path[i] &lt;- path[i-1] / 2\n}\nggplot(data.frame(x = path, y = 0)) +\n    geom_segment(aes(x = x, y = 0, xend = x, yend = 1)) +\n    theme_vs() +\n    theme(axis.text.y = element_blank()) +\n    xlab('Distance') + ylab('') + \n    scale_x_reverse()\n\n\n\n\n\n\n\n\nFigure A.2: Zeno’s paradox of dichotomy.\nConsider the \\(\\chi^2\\) (chi-squared) distribution with 8 degrees of freedom (Figure A.3). The tail to the right never actually crosses (or touches) the line x = 0. However, from calculus we know that the limit of \\(\\chi^2\\) as x approaches infinity is zero.\nShow the code\nggplot() + \n    stat_function(fun = dchisq, args = list(df = 8)) +\n    ylab('') + xlab('') +\n    xlim(c(0, 50)) +\n    theme_vs()\n\n\n\n\n\n\n\n\nFigure A.3: Chi-squared distribution with 8 degrees of freedom.\nWe write this mathematically as:\n\\[ \\lim_{x\\to\\infty} \\chi^2_{df = 8}(x) = 0  \\tag{A.4}\\]\nAs discussed in the distributions section, a key feature of statistical distributions (e.g. normal, F, t, etc.) is that the area under the curve is one. However the length of the distribution line is infinite (in one direction for one tailed tests, in both directions for two tailed tests), but the area is finite. When this occurs we say that the function converges. With a \\(\\chi^2\\) distribution as depicted in Figure A.3, test statistics do not need to be very large before the y-value is smaller than our numeric precision. For example, the y-value for a \\(\\chi^2\\) distribution where x = 50 with 8 degrees of freedom \\(1.8 \\times 10^{-8}\\).\nShow the code\ndchisq(50, 8)\n\n\n[1] 1.808326e-08\nWith regard to statistics, understanding the limit is important for the central limit theorem which states that as n (i.e. sample size) increases (approaches infinity), the sampling distribution will approximate the normal distribution. But the limit is also important to understand the concepts of derivatives and integrals, which are presented in the next two sections.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Calculus</span>"
    ]
  },
  {
    "objectID": "calculus.html#limits",
    "href": "calculus.html#limits",
    "title": "Appendix A — Calculus",
    "section": "",
    "text": "The first asserts the non-existence of motion on the ground that that which is in locomotion must arrive at the half-way stage before it arrives at the goal.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Calculus</span>"
    ]
  },
  {
    "objectID": "calculus.html#derivatives",
    "href": "calculus.html#derivatives",
    "title": "Appendix A — Calculus",
    "section": "A.2 Derivatives",
    "text": "A.2 Derivatives\nThe derivative of a function describe the instantaneous rate of change at a certain point. Consider the example in Figure A.1, the derivative for the distance function describes velocity, and the derivative of the velocity function describes acceleration. Visually, the derivative is the slope of the line tangent the desired point.\n\n\nShow the code\nderivative_plot(fun = normal_fun, x_value = 1, delta_x = 0.5)\n\n\n\n\n\nEstimated derivative at x = 1 for the normal distribution with a delta x of 0.5.\n\n\n\n\n\n\nShow the code\nderivative_plot(fun = normal_fun, x_value = 1, delta_x = 0.1)\n\n\n\n\n\nEstimated derivative at x = 1 for the normal distribution with a delta x of 0.1.\n\n\n\n\nValues where the first derivative (i.e. slope) is zero provide a way of finding local minumums and maximums.\n\n\nShow the code\nf &lt;-  expression(x^2 - 2 * x + x )\nd &lt;- D(f, \"x\")\nfirst_deriv &lt;- uniroot(function(x) { eval(D(f, \"x\"))}, lower = -3, upper = 3)\nggplot(data.frame()) +\n    geom_function(fun = function(x) { eval(f) }) + \n    geom_hline(yintercept = eval(f, envir = list(x = first_deriv$root)), linetype = 2) +\n    geom_point(x = 1, y = -1, size = 3, color = 'blue') +\n    xlim(c(-1, 3)) +\n    xlab('x') + ylab('f(x)') +\n    theme_vs()\n\n\n\n\n\n\n\n\nFigure A.4: Function of \\(x^2 - 2 * x + x\\)\n\n\n\n\n\n\n\nShow the code\nf &lt;-  expression(-x^3 - 3 * x^2 + 2  )\ndf &lt;- D(f, \"x\")\nf1 &lt;- function(x) { -(3 * x^2 + 3 * (2 * x)) }\nuniroot(f1, lower = -3, upper = 3, tol = 1e-9)\nggplot(data.frame()) +\n    geom_function(fun = function(x) { -x^3 - 3 * x^2 + 2 }) + \n    # geom_hline(yintercept = -1, linetype = 2) +\n    # geom_point(x = 1, y = -1, size = 3, color = 'blue') +\n    xlim(c(-3.2, 1.2)) +\n    xlab('x') + ylab('f(x)') +\n    theme_vs()",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Calculus</span>"
    ]
  },
  {
    "objectID": "calculus.html#integrals",
    "href": "calculus.html#integrals",
    "title": "Appendix A — Calculus",
    "section": "A.3 Integrals",
    "text": "A.3 Integrals\n\n\nShow the code\nintegral_plot(fun = normal_fun, xmin = 0, xmax = 1, n = 3)\n\n\n\n\n\nEstimated integral using Reimann sums with 3 rectangles.\n\n\n\n\n\n\nShow the code\nintegral_plot(fun = normal_fun, xmin = 0, xmax = 1, n = 10)\n\n\n\n\n\nEstimated integral using Reimann sums with 10 rectangles.\n\n\n\n\n\n\nShow the code\nintegral_plot(fun = normal_fun, xmin = 0, xmax = 1, n = 100)\n\n\n\n\n\nEstimated integral using Reimann sums with 100 rectangles.\n\n\n\n\n\n\n\n\nAristotle. 1929. Physics. Translated by P. H. Wicksteed and F. M. Cornford. New York: G. P. Putnam.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Calculus</span>"
    ]
  }
]